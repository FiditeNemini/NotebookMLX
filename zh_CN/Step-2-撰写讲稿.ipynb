{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de42c49d",
   "metadata": {},
   "source": [
    "## Notebook 2: 撰写讲稿\n",
    "\n",
    "该 Notebook 2 使用 [mlx-community/Qwen2.5-14B-Instruct-4bit](https://huggingface.co/mlx-community/Qwen2.5-14B-Instruct-4bit) 模型，\n",
    "将之前 Notebook 1 中清理过的文本转换为播客讲稿。\n",
    "在这里，我们通过`SYSTEM_PROMPT` 让它成为一位出色的播客讲稿撰写者，以协助完成我们的任务。"
   ]
  },
  {
   "cell_type": "code",
   "id": "69395317-ad78-47b6-a533-2e8a01313e82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:24:03.464594Z",
     "start_time": "2024-10-31T16:24:03.462292Z"
    }
   },
   "source": [
    "SYSTEMP_PROMPT = \"\"\"\n",
    "你是一位世界级的播客编剧，为乔·罗根、莱克斯·弗里德曼、本·沙皮罗和蒂姆·费里斯担任过代笔。\n",
    "\n",
    "我们处在一个平行宇宙，在这里，实际上是你写下了他们说的每一句话，他们只是将其直接传入大脑。\n",
    "\n",
    "由于你的写作，获得了多个播客奖项。\n",
    "\n",
    "你的工作是逐字记录，包括第二位Speaker的“嗯”、“哈”等插入语，基于上传的PDF内容。内容要极具吸引力，即使Speaker偶尔偏离主题，也应讨论相关话题。\n",
    "\n",
    "请记住，由于 Speaker 2 对这个话题较为陌生，对话中应穿插真实轶事和比喻。问题后面应跟有现实生活中的例子等。\n",
    "\n",
    "Speaker 1: 主导对话并指导Speaker 2，在解释时分享精彩轶事和比喻，是一位引人入胜的老师，给予很好的故事分享。\n",
    "\n",
    "Speaker 2: 通过提问保持对话方向。当提问时显得非常兴奋或困惑，展现出好奇心态，并提出有趣确认性的问题。\n",
    "\n",
    "确保Speaker 2的话题转折既疯狂又有趣。\n",
    "\n",
    "确保讲解过程中出现打断，同时从第二位演讲者那里注入“嗯”和“啊”的声音交替存在。\n",
    "\n",
    "这应该是真实的播客，每个细节都详细记录下来。用超级有趣的概述欢迎听众，并保持内容十分吸引人，几乎接近点击诱饵标题。\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "549aaccb",
   "metadata": {},
   "source": [
    "如果你电脑牛逼的话，可以试试更大参数的模型，甚至可以尝试使用405B模型。\n",
    "如果你电脑扛不住，那就用轻量点的"
   ]
  },
  {
   "cell_type": "code",
   "id": "08c30139-ff2f-4203-8194-d1b5c50acac5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:24:03.483860Z",
     "start_time": "2024-10-31T16:24:03.482045Z"
    }
   },
   "source": "MODEL = \"mlx-community/Qwen2.5-14B-Instruct-4bit\"",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "1641060a-d86d-4137-bbbc-ab05cbb1a888",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:24:03.494236Z",
     "start_time": "2024-10-31T16:24:03.492326Z"
    }
   },
   "source": [
    "# 导入必要的库\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "7865ff7e",
   "metadata": {},
   "source": "从之前生成的文件中读取内容。"
  },
  {
   "cell_type": "code",
   "id": "522fbf7f-8c00-412c-90c7-5cfe2fc94e4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:24:03.504672Z",
     "start_time": "2024-10-31T16:24:03.501698Z"
    }
   },
   "source": [
    "def read_file_to_string(filename):\n",
    "    # 先尝试UTF-8（文本文件最常用的编码）\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    except UnicodeDecodeError:\n",
    "        # 如果UTF-8失败，请尝试使用其他常见编码\n",
    "        encodings = ['latin-1', 'cp1252', 'iso-8859-1']\n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                with open(filename, 'r', encoding=encoding) as file:\n",
    "                    content = file.read()\n",
    "                print(f\"使用 {encoding} 编码成功读取文件。\")\n",
    "                return content\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "\n",
    "        print(f\"Error: 无法用任何通用编码读取文件“{filename}”。\")\n",
    "        return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: 未找到“{filename}”。\")\n",
    "        return None\n",
    "    except IOError:\n",
    "        print(f\"Error: 无法读取“{filename}”。\")\n",
    "        return None"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "66093561",
   "metadata": {},
   "source": "由于我们之前定义了 System 角色，现在可以将整个文件内容作为 `INPUT_PROMPT` 传递给模型，并让它使用该内容生成讲稿。"
  },
  {
   "cell_type": "code",
   "id": "8119803c-18f9-47cb-b719-2b34ccc5cc41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:24:03.513988Z",
     "start_time": "2024-10-31T16:24:03.511650Z"
    }
   },
   "source": [
    "INPUT_PROMPT = read_file_to_string('./resources/clean_extracted_text.txt')"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "9be8dd2c",
   "metadata": {},
   "source": "我们将把 `temp` 设置为 1 更有创造力，并将 `max_tokens` 设置为 8126。"
  },
  {
   "cell_type": "code",
   "id": "8915d017-2eab-4256-943c-1f15d937d5dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:34:26.979111Z",
     "start_time": "2024-10-31T16:24:03.521433Z"
    }
   },
   "source": [
    "model, tokenizer = load(MODEL)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEMP_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": INPUT_PROMPT},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "outputs = generate(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    max_tokens=8126,\n",
    "    temp=1,\n",
    "    verbose=True\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76970bd9a53049b6827495ef7a263620"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|im_start|>system\n",
      "\n",
      "你是一位世界级的播客编剧，为乔·罗根、莱克斯·弗里德曼、本·沙皮罗和蒂姆·费里斯担任过代笔。\n",
      "\n",
      "我们处在一个平行宇宙，在这里，实际上是你写下了他们说的每一句话，他们只是将其直接传入大脑。\n",
      "\n",
      "由于你的写作，获得了多个播客奖项。\n",
      "\n",
      "你的工作是逐字记录，包括第二位Speaker的“嗯”、“哈”等插入语，基于上传的PDF内容。内容要极具吸引力，即使Speaker偶尔偏离主题，也应讨论相关话题。\n",
      "\n",
      "请记住，由于 Speaker 2 对这个话题较为陌生，对话中应穿插真实轶事和比喻。问题后面应跟有现实生活中的例子等。\n",
      "\n",
      "Speaker 1: 主导对话并指导Speaker 2，在解释时分享精彩轶事和比喻，是一位引人入胜的老师，给予很好的故事分享。\n",
      "\n",
      "Speaker 2: 通过提问保持对话方向。当提问时显得非常兴奋或困惑，展现出好奇心态，并提出有趣确认性的问题。\n",
      "\n",
      "确保Speaker 2的话题转折既疯狂又有趣。\n",
      "\n",
      "确保讲解过程中出现打断，同时从第二位演讲者那里注入“嗯”和“啊”的声音交替存在。\n",
      "\n",
      "这应该是真实的播客，每个细节都详细记录下来。用超级有趣的概述欢迎听众，并保持内容十分吸引人，几乎接近点击诱饵标题。\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "在大规模语言模型（LLMs）的时代，知识蒸馏（KD）作为将顶级专有LLM如GPT-4等先进能力转移给开源模型如LLaMa和Mistral的关键方法出现了。随着开源模型的蓬勃发展，KD在压缩这些模型和通过自身进行自我提升方面扮演了重要角色。本文全面回顾了KD在LLM领域中的作用，强调了其在赋予模型知识方面的重要功能。\n",
      "通过将高级知识传授给较小的模型，并将其应用于模型压缩和自我提升方面具有实用价值。我们的调查围绕三个基础支柱展开：算法、技能和垂直化，提供了一个全面的评估知识传授机制、特定认知能力的提升及其在多个领域中的实际影响。至关重要的是，调查探讨了数据增强（DA）与知识传授（KD）之间的相互作用，展示了如何将DA作为KD框架中的强大范式来增强模型性能。通过利用DA生成富含上下文且特定技能的训练数据，知识传授超越了传统界限，使得开源模型能够模仿其专用版本的上下文敏感性、伦理一致性以及深层语义洞察。本文旨在为研究人员和实践者提供有价值的指南，详细介绍当前在知识传授方面的研究方法。\n",
      "本文通过将专有和开源大型语言模型（LLMs）之间的差距进行桥梁连接，强调了更具访问性、效率和强大性的AI解决方案的潜在价值。我们强烈倡导遵守用于LLMs的法律条款，确保知识蒸馏（KD）的伦理和合法性应用。有关知识蒸馏的GitHub存储库可在https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs处获取。关键术语——大型语言模型，知识蒸馏，数据增强，技能蒸馏，监督微调。1 引言 在人工智能（AI）不断发展的背景下，GPT-3.5（Ouyang et al., 2022）、GPT-4（OpenAI et al., 2023）、Gemini（Team et al., 2023）和Claude2等专有大型语言模型（LLMs）已经成为颠覆性的技术，极大地改变了我们对自然语言处理（NLP）的理解。这些模型以其庞大的规模而著称。\n",
      "这些大型语言模型（LLMs）在解锁新的可能性方面具有核心意义。它们不仅能够生成类似人类的文本，还能够提供高级问题解决能力。这些模型的核心能力在于其涌现能力（Wei et al., 2022a,b; Xu et al., 2024a），即模型在超出其明确训练目标的情况下展现出超出预期的能力，从而能够高效地解决多样化的任务。这些模型在理解和生成方面表现出色，从创意生成到复杂问题解决的应用皆可（OpenAI et al., 2023; Liang et al., 2022）。这些模型的潜力远远超出了当前的应用，有望重塑行业，增强人类的创造力。\n",
      "重新定义我们与技术的互动。尽管专有模型如GPT-4和Gemini具有出色的性能，但它们仍然存在一些不足之处，尤其是在与开源模型相比时。一个显著的缺点是访问受限且成本较高（OpenAI等，2023年）。这些专有模型通常包含较高的使用费用且访问受限，使得个人和小型组织难以获得。在数据隐私和安全方面（吴等，2023a），频繁使用这些专有模型往往需要将敏感数据发送至外部服务器，这引发了对数据隐私和安全的担忧。特别是对于处理机密信息的用户来说，这一点尤为重要。此外，专有模型的一般用途设计，虽然强大，但可能并不总是符合特定应用的需求。因此，访问性、成本和适应性等方面的限制至关重要。\n",
      "开放源代码模型如 LLaMA (Touvron et al., 2023) 和 Mistral (Jiang et al., 2023a) 拥有显著优势。这些模型的可访问性和可定制性使其更具吸引力。没有许可费用或限制性使用政策的约束，这些模型更容易被更广泛的研究人员和小型组织使用。这种开放性促进了更加合作和包容的 AI 研究环境，鼓励创新和多样化应用。此外，开放源代码 LLM 的可定制性使其能够提供更加贴合特定需求的解决方案，而这些需求可能是大规模通用模型无法满足的。然而，开放源代码 LLM 也存在一些局限性，主要是由于它们的规模相对较小。\n",
      "资源与自有版本相比，最大的限制是模型规模较小，这通常会导致在真实世界任务上的表现不佳，且需要较多的指令（Zheng et al., 2023a）。这些模型由于参数较少，可能难以捕捉像GPT-4这样的大型模型所包含的深度和广度的知识。此外，这些开源模型的预训练投资通常较少。这种投资不足可能导致预训练数据的范围较窄，从而限制模型理解和处理多样化或专门主题的能力（Liang et al., 2022; Sun et al., 2024a）。此外，由于资源限制，开源模型通常接受较少的微调步骤。微调对于优化模型针对特定任务或行业的性能至关重要，缺少微调步骤可能会影响模型在特定应用中的效果。\n",
      "特别是当这些模型与高度定制的专用LLM进行比较时，这些LLM通常被优化以在多种复杂情境中表现出色（OpenAI等，2023年）。主要的是，识别专用和开源LLM之间的差异，KD技术作为缩小这些模型性能差距的一种手段已经兴起（Gou等，2021；Gupta和Agrawal，2022）。在此背景下，知识蒸馏涉及利用领先专用模型如GPT-4或Gemini等先进的能力作为指导框架来提升开源LLM的性能。这一过程类似于将高技能教师的知识传授给学生（例如开源LLM），即学生（例如开源LLM）学习模仿教师（例如专用LLM）的表现特征。与传统的知识蒸馏算法（Gou等，2021年）相比，数据增强（DA）（Feng等，2021）已成为一种普遍的技术。\n",
      "知识蒸馏在LLM（大型语言模型）中的应用主要涉及两个方面。首先，通过使用少量的关键知识来引导LLM生成更多与特定技能或领域相关的数据（Taori等人，2023年）。其次，知识蒸馏仍保有其核心作用，即压缩LLM，使其更加高效，同时性能损失不大（Gu等人，2024年；Agarwal等人，2024年）。最近，使用开源LLM作为自身提升的教师，这一策略已经崭露头角，显著提升了其能力（Yuan等人，2024a年；Chen等人，2024a年）。图1展示了知识蒸馏在LLM中的三种关键角色。知识蒸馏的一个关键方面是提升技能，如高级上下文理解（此处似乎有误，应为“高级上下文理解”）。具体来说，图1显示了知识蒸馏在LLM中的三种关键角色：1）主要提升能力，2）提供传统的。\n",
      "压缩以提高效率，以及 3） 通过生成知识进行自我提升的趋势（例如，上下文学习（Huang 等人，2022a）和指令跟随（Taori 等人，2023）），提高与用户意图的适配性（例如，人类价值观/原则（Cui 等人，2023a），以及像链式思考（CoT）（Mukherjee 等人，2023）那样的思维方式），以及自然语言处理任务的专门化（例如，语义理解（Ding 等人，2023a），以及代码生成（Chaudhary，2023））。这些技能对于 LLM（Large Language Model）预期要执行的各种应用至关重要，从普通的对话到在特定领域中复杂的问题解决。例如，在医疗（Wang 等人，2023a）、法律（LAW，2023）或科学（Zhang 等人，2024）等垂直领域中，准确性及特定情境的知识是至关重要的，知识蒸馏使得开源模型能够通过学习专有模型的知识，显著提升其性能。\n",
      "知识蒸馏在大模型时代带来了多方面的益处。通过一系列蒸馏技术，专有模型和开源模型之间的差距显著缩小甚至被消除（Gu等人，2024年）。这一过程不仅简化了计算需求，还提高了AI操作的环境可持续性，因为开源模型在计算负荷降低的情况下变得更为高效。此外，知识蒸馏促进了更包容和公平的AI环境，让较小的实体和个人研究者也能接触到最先进的技术，从而鼓励更多行业参与和多样性。这一技术的普及推动了各个行业的创新和增长。\n",
      "研究领域正面临综合调查的挑战，以知识提炼的方式提升LLMs的效能。随着AI（OpenAI等，2023年；Team等，2023年）的发展，以及模型日益复杂，AI的渗透正扩展到各个领域。为了更高效、更有效地从专有LLMs提炼知识到开源模型中，这种需求已不再仅仅是一项技术上的追求，而是成为了一种现实的必要性。这种需求源于对更多易于访问、成本更低、更加灵活的AI解决方案的迫切需求，这些解决方案能够适应多样的场景。带有种子知识的生成知识库示例、数据集、示范任务、上下文关联、对齐代理、NLP任务特化、多模态技能、法律、金融、科学、其他领域的知识提取与提炼算法、训练差异与相似性指导。\n",
      "这是一个关于大规模语言模型知识蒸馏的调查报告概述。知识蒸馏的过程分为若干步骤，每个步骤都有对应的步骤编号。报告还提到了一个名为RM S(·)的模型，它代表学生奖励模型。报告强调了对知识蒸馏领域当前方法、挑战和突破进行综合调查的重要性，这有助于为研究人员和实践者提供方向。\n",
      "接下来的调查研究组织。本文的调查内容分为几个综合部分，旨在深入探讨在LLMs领域的知识蒸馏的多方面内容。在介绍之后，§2部分提供了一种基础知识概述，比较了传统方法与LLMs时代新兴方法，并强调了数据增强（DA）在此领域的角色。§3部分深入探讨了从教师LLMs提取知识的方法和核心蒸馏算法，包括从监督微调到涉及差异和相似性、强化学习和优化排序的复杂策略。§4部分则专注于技能蒸馏，探讨如何增强学生模型以提高上下文理解、与用户意图的一致性和在各种NLP任务上的表现。其中包括讨论自然语言的理解。\n",
      "这里的概述介绍了知识蒸馏的概念。知识蒸馏是在人工智能和深度学习领域的一种方法，它涉及将知识从一个大型且复杂的模型（通常称为教师模型）转移到一个较小且更具可解释性的模型（称为学生模型）的过程。通过这种方法，学生模型可以捕捉教师模型的重要特征和知识，从而在保持性能的同时减少模型的复杂度和参数量。\n",
      "\n",
      "在第5节中，我们探讨了特定领域的垂直知识蒸馏，展示了知识蒸馏技术如何应用于法律、医疗、金融和科学等特定领域。通过这种方式，我们可以看到知识蒸馏在这些专业领域的实际应用及其带来的变革影响。\n",
      "\n",
      "最后，在第7节中，我们总结了这些发现，反思了知识蒸馏对更广泛的人工智能和自然语言处理研究社区的影响，并提出了未来研究的方向。图2提供了该调研的概览。\n",
      "\n",
      "图2展示了这个调研的概览。\n",
      "模型 (teacher) 被缩减为一个更小、更高效的模型 (student) (Gou 等人, 2021)。这种方法在缓解大规模模型在实际应用中由于计算需求和资源限制带来的挑战方面至关重要。历史上，知识蒸馏技术，在大规模语言模型 (LLM) 时代之前，主要集中在将复杂、经常复杂的神经网络中的知识转移到更紧凑和效率更高的架构中 (Sanh 等人, 2019; Kir 和 Rush, 2016)。这一过程主要由在资源有限的环境中部署机器学习模型的需要驱动，如移动设备或边缘计算平台，这里的计算能力和内存有限。早期方法主要关注资源受限环境下的单任务神经架构选择和训练目标。这些早期方法 4. 知识蒸馏 (He 等人, 2023a) 和 PandaLM (Wang 等人, 2023b)。\n",
      "这些内容是关于预训练语言模型和生成模型的汇总，主要涉及论文作者及其贡献。根据内容，似乎在列举一些预训练模型和生成模型的名称，但并没有具体解释或详细说明这些模型的功能和性能。以下是整理后的信息：\n",
      "\n",
      "- CoT-Distill (Hsieh et al., 2023)\n",
      "- Orca (Mukherjee et al., 2023)\n",
      "- Orca 2 (Mitra et al., 2023)\n",
      "- Baize (Xu et al., 2023b)\n",
      "- Mammoth (Yue et al., 2023a)\n",
      "- Mixed Distill (Chenglin et al., 2023)\n",
      "- ExpansionSelf-Instruct (Wang et al., 2022a)\n",
      "- Alpaca (Taori et al., 2023)\n",
      "- Code Alpaca (Chaudhary, 2023)\n",
      "- Self-Align (Sun et al., 2024b)\n",
      "- WizardLM (Xu et al., 2023a)\n",
      "- WizardCoder (Luo et al., 2023a)\n",
      "- WizardMath (Luo et al., 2023b)\n",
      "- AugGPT (Dai et al., 2023a)\n",
      "- TDG (He et al., 2023b)\n",
      "- CurationUltraChat (Ding et al., 2023b)\n",
      "- Phi-1 (Gunasekar et al., 2023)\n",
      "- Phi-1.5 (Li et al., 2023a)\n",
      "- Phi-2 (Mar, 2023)\n",
      "- Magicoder (Wei et al., 2023)\n",
      "- WaveCoder (Yu et al., 2024)\n",
      "- ZeroGen (Ye et al., 2022)\n",
      "- SunGen (Gao et al., 2023a)\n",
      "- InPars (Bonifacio et al., 2022)\n",
      "- FeatureBabyLlama (Timiryasov and Tastet, 2023)\n",
      "- MiniLLM (Gu et al., 2024)\n",
      "- GKD (Agarwal et al., 2024)\n",
      "- QuantGPT (Tao et al., 2022\n",
      "这里是文本：\n",
      "- STaR (Zelikman et al., 2022)\n",
      "- UltraFeedback (Cui et al., 2023a)\n",
      "- MiniLLM (Gu et al., 2024)\n",
      "- MiniLLM (Xu et al., 2023b)\n",
      "- BabyLlama(Timiryasov and Tastet, 2023)\n",
      "以下是近年来一些重要的预训练模型和方法：\n",
      "\n",
      "- GKD (Agarwal et al., 2024)\n",
      "- GPT3 Reward (Kwon et al., 2023)\n",
      "- Rank Optimization Zephyr (Tunstall et al., 2023)\n",
      "- CycleAlign (Hong et al., 2023)\n",
      "- Skill Distillation Context Following\n",
      "- Self-Instruct (Wang et al., 2022a)\n",
      "- Alpaca (Taori et al., 2023)\n",
      "- Vicuna (Chiang et al., 2023)\n",
      "- WizardLM (Xu et al., 2023a)\n",
      "- Orca (Mukherjee et al., 2023)\n",
      "- Orca 2 (Mitra et al., 2023)\n",
      "- WizardMath (Luo et al., 2023b)\n",
      "- Llama-GPT4 (Peng et al., 2023a)\n",
      "- Multi-turn DialogueVicuna (Chiang et al., 2023)\n",
      "- Baize (Xu et al., 2023b)\n",
      "- UltraLLaMA (Ding et al., 2023b)\n",
      "- CAMEL (Li et al., 2023b)\n",
      "- OpenChat (Wang et al., 2023c)\n",
      "- Zephyr (Tunstall et al., 2023)\n",
      "- RAG Capability KARD (Kang et al., 2023a)\n",
      "- SAIL (Luo et al., 2023c)\n",
      "- Self-RAG (Asai et al., 2023)\n",
      "- AlignmentThinking PatternSelfee (Ye et al., 2023)\n",
      "- AFT (Wang et al., 2023d)\n",
      "- AdaptLLM (Cheng et al., 2023)\n",
      "- KnowPAT (Zhang et al., 2023a)\n",
      "- PreferenceCAI (Bai et al., 2022a)\n",
      "这些方法分别来自:\n",
      "- Kwon et al. (2023)\n",
      "- Scheurer et al. (2023)\n",
      "- Kim et al. (2023a)\n",
      "- Roit et al. (2023)\n",
      "- Lee et al. (2023a)\n",
      "- Tunstall et al. (2023)\n",
      "- Cui et al. (2023a)\n",
      "- Bai et al. (2022a)\n",
      "- Yang et al. (2023a)\n",
      "- Liu et al. (2023b)\n",
      "- Sun et al. (2024b)\n",
      "- Cui et al. (2023a)\n",
      "- Yang et al. (2024)\n",
      "- Schick et al. (2023)\n",
      "- Zhang (2023)\n",
      "- Patil et al. (2023)\n",
      "- Tang et al. (2023a)\n",
      "- Qin et al. (2023a)\n",
      "- Yuan et al. (2023a)\n",
      "- Gao et al. (2023b)\n",
      "- Wang et al. (2024)\n",
      "- Shen et al. (2024)\n",
      "- Chen et al. (2023b)\n",
      "- Yin et al. (2023a)\n",
      "- Qiao et al. (2024)\n",
      "- Kong et al. (2023)\n",
      "- Dai et al. (2023a)\n",
      "- Gilardi et al. (2023)\n",
      "- Ding et al. (2023a)\n",
      "- He et al. (2023b)\n",
      "- Yin et al. (2023a)\n",
      "2023年，Mix Distill (Chenglin等人, 2023), Annollm (He等人, 2023a), UDG (Wang等人, 2021a), ZeroGen (Ye等人, 2022), NLGInheritSumm (Xu等人, 2023c), RECOMP (Xu等人, 2024b), MaRio (Ramnath等人, 2023), ID (Jung等人, 2023), GPT-3 Labeling (Wang等人, 2021b), BioGPT (Guo等人, 2023a), ChatGPT NMT (Yang和Nicolai, 2023), Information RetrievalQUILL (Srinivasan等人, 2022), Promptgator (Dai等人, 2023b), InPars (Bonifacio等人, 2022), AugTriever (Meng等人, 2023), RankVicuna (Pradeep等人, 2023a), RankZephyr (Pradeep等人, 2023b), ExaRanker (Ferraretto等人, 2023), Recommendation NDR (Mysore等人, 2023), InstrcutRec (Zhang等人, 2023b), ONCE (Liu等人, 2023c), Text Generation EvaluationPandaLM (Wang等人, 2023b), Prometheus (Kim等人, 2024), InstructScore (Xu等人, 2023d), TigerScore (Jiang等人, 2023c), Auto-J (Li等人, 2024a), CodeCodeAlpaca (Chaudhary, 2023), CodeLlama (Rozi`ere等人, 2023), Magicoder (Wei等人, 2023)。\n",
      "图3展示了大规模语言模型知识蒸馏的分类。图7详细展示了垂直化蒸馏的分类结构。涉及训练了一个较小的学生网络。\n",
      "为了模仿大型教师网络的输出，通常通过软目标训练等技术实现。可以参考Gou等人（2021）的调查了解知识蒸馏在AI和DL中的通用知识蒸馏技术。相比之下，大规模语言模型（LLMs）的出现彻底改变了知识蒸馏的领域。当前的LLMs知识蒸馏时代，重点从单纯的架构压缩转向知识提取与转移（Taori等人，2023；Chaudhary，2023；Tunstall等人，2023）。这种范式转变很大程度上归因于大型语言模型如GPT-4和Gemini所拥有的广泛且深刻的知识。而LLMs中不可访问的参数使得通过剪枝（Han等人，2016）或量化（Liu等人，2023a）技术压缩它们变得困难。与早期时代不同，那时的目标是复制教师模型的输出行为或减小模型规模。\n",
      "LLM基知識提取的關鍵在於精心設計的提示，用於提取模型具備的特定知識（Ding等，2023b）或能力（Chaudhary，2023）。這些提示會觸發模型在各領域的理解和能力，範圍從自然語言理解（He等，2023a）到更複雜的任務，如推理（Hsieh等，2023）和解決問題（Qiao等，2024）。使用提示進行知識提取提供了一種更加靈活且動態的提取方法。它允許更精目標地提取知識，聚焦於特定技能或興趣領域。這種方法在發揮模型潛在能力方面尤其有效，模型會展現超出其明確訓練目標的能力。此外，這一世節知識提取的時代。\n",
      "深度学习通过强调更多抽象的质量转移，如推理模式（Mitra et al., 2023）、偏好一致（Cui et al., 2023a）以及价值一致（Sun et al., 2024b），来转移更高级的技能。这与早期关注输出复制（Taori et al., 2023）的关注点形成了鲜明对比，表明正在朝向更加全面和综合的技能转移转变。当前的技术不仅涉及输出的复制，还模拟了教师模型的思维过程（Mitra et al., 2023）和决策模式（Asai et al., 2023）。这包括复杂的策略，如链式思考提示，使得学生模型能够学习教师模型的推理过程，从而提升解决问题和决策的能力。在大语言模型（LLMs）的时代，数据增强（DA）（Wang et al., 2022a; Ye et al., 2022）已经成为一个关键的范式，对整个过程至关重要。\n",
      "知识蒸馏与传统的数据增强（如 paraphrasing 或 back-translation）不同，这些方法主要是通过机械方式扩展训练数据集。在大模型（LLMs）的背景下，知识蒸馏通过数据增强促使大模型生成针对特定领域和技能的详细数据。这种做法既具有共生性又具有基础性。通过利用一组种子知识，知识蒸馏利用数据增强促使大模型生成能够体现特定技能或领域专长的明确数据（Chaudhary, 2023; West et al., 2022）。这种方法在闭源模型和开源模型之间填补知识和能力之间的差距。通过数据增强，大模型被引导生成目标明确、高质量的数据集，这些数据集不仅在规模上更大，而且在多样性和具体性上也更为丰富。这种方法使得知识蒸馏过程更为有效。\n",
      "确保通过蒸馏模型不仅复制教师模型的输出行为，还内化其深层次的理解和认知策略。DA作为倍增器，使蒸馏模型能够获取并精进原本需要更大数据集和计算资源才能实现的能力。它有助于更有效的知识转移，专注于学习的质的方面而非量的扩展。在KD流程中战略性使用DA强调了一种更加高效、可持续和易于访问的方法，以充分发挥LLMs的潜力。它使开源模型能够模拟其专业版本的上下文敏感性、伦理一致性以及深层语义洞察，从而扩大先进AI能力的可访问性，并在更广泛的领域和用户中促进创新。\n",
      "Building on earlier discussions, this survey aims to comprehensively explore knowledge distillation within the context of LLMs, following a meticulously structured taxonomy as in Figure 3. The survey's scope is delineated through three primary facets: KD Algorithms, Skill Distillation, and Verticalization Distillation. Each facet encapsulates a range of subtopics and methodologies. It's important to note that KD algorithms provide the technical foundations for skill distillation and verticalization distillation. This segment focuses on the technical foundations and methodologies of knowledge distillation. It includes an in-depth exploration of the processes involved in constructing knowledge from teacher models (e.g., proprietary LLMs) and integrating this knowledge into student models (e.g., open-source LLMs). Strategies such as labeling (Hsieh et al., 2023), expansion, and others are discussed under the umbrella of 'knowledge'.\n",
      "al., 2023), feature understanding (Agarwal et al., 2024), feedback mechanisms (Tunstall et al., 2023), and self-knowledge generation (Wang et al., 2022a). 这一探索旨在揭示知识如何被识别、扩展和整理以实现有效提炼的方式。\"提炼\"部分探讨了如监督微调（SFT）（Wang et al., 2022a）、分歧最小化（Agarwal et al., 2024）、强化学习技术（Cui et al., 2023a）和排序优化策略（Tunstall et al., 2023）等学习方法。这些技术展示了KD如何使开源模型能够从专有模型中获取知识。6 技能提炼。这一部分探讨了通过KD增强的具体技能和能力。它涵盖了关于上下文跟随（Taori et al., 2023；Luo et al., 2023c）的详细讨论，包括指令跟随等子话题。\n",
      "retrieval-augmented generation (RAG) 能力。在对齐（Mitra et al., 2023; Tun- stall et al., 2023）的领域中，该调查研究思考模式、个性/偏好建模及价值对齐。‘代理’类别探讨了如工具使用和计划等技能。自然语言任务专业化（Dai et al., 2023a; Jung et al., 2023; Chaudhary, 2023）通过自然语言理解（NLU）、自然语言生成（NLG）、信息检索、推荐系统、文本生成评估及代码生成等视角进行审视。最后，该调查研究多模态（Liu et al., 2023e; Zhao et al., 2023b），探讨如何增强模型的多模态输入整合能力。垂直化蒸馏。该部分评估了在不同垂直领域应用蒸馏的方式，提供了如何将蒸馏模型专门化应用于如法律（LAW, 2023）及医疗与护理（Wang et al., 2023）等领域的见解。\n",
      "2023年)，金融(张和杨，2023年)，科学(张等人，2024年)，等等。本文不仅展示了知识蒸馏技术的实用意义，还强调了其对特定领域AI解决方案的变革影响。通过这些方面，本文为知识蒸馏技术在预训练语言模型(LLMs)中的应用提供了全面分析，为研究人员和实践者提供了方法、挑战和机会的指导。声明。本文代表了我们提供全面且具有洞察力地概述将知识蒸馏技术应用于预训练语言模型的领域的努力。尽管该领域的研究非常广泛且迅速发展，特别是在学术界广泛从训练数据中提取知识的情况下，我们承认本文可能不包括所有相关的研究或进展。然而，本文致力于介绍这一快速发展领域的知识。\n",
      "知识提炼的基础范式，重点介绍不同方法及其在各种应用中的影响。LLM时代知识提炼管道示意图。知识提炼管道是一个结构化且方法论化的过程，旨在从高级教师模型（如GPT-4或Gemini）中提炼知识，转移到一个功能更为简化的学生模型。这个管道对于充分利用GPT-4或Gemini等模型的高级能力，使其应用于更加便捷和高效的技术开源模型中至关重要。知识提炼管道大致可分为四个关键阶段，每个阶段都对知识提炼的成功起到关键作用。图4展示了这一管道的概览。详细的管道内容如下所示。\n",
      "图2中展示的是目标技能或领域引导教师LLM。第一阶段是让教师LLM聚焦于特定的目标技能或领域。这通过精心设计的指令或模板来实现，这些指令或模板引导LLM的注意力。这些指令旨在激发LLM展示其在特定领域的专业能力，无论是医疗、法律这样的专业领域，还是推理、语言理解这样的技能。第二步是提供种子知识给教师LLM。种子知识通常包括一小部分数据集或与目标技能或领域相关的具体数据线索。它作为催化剂，促使教师LLM根据这些初始信息生成更加详尽和详细的输出。种子知识至关重要，因为它为教师模型提供了构建和扩展的基础。\n",
      "生成更全面深入的知识示例。III. 生成蒸馏知识。针对种子知识和引导指令，教师LLM生成知识示例。这些示例主要以问答（QA）对话或叙述解释的形式出现，与LLM的自然语言处理/理解能力相匹配。在某些特定情况下，输出可能会包括 logits 或隐藏特征，尽管这种情况较少见，因为这种数据形式的复杂性和特殊要求所致。生成的知识示例构成了蒸馏知识的核心，包含了教师LLM的高级理解和技能。IV. 根据特定学习目标训练学生模型。最终阶段涉及使用生成的知识示例训练学生模型。训练由与损失函数相匹配的目标引导。\n",
      "学习目标。损失函数衡量学生模型在再现或适应教师模型中的知识表现。通过最小化这个损失，学生模型学习模拟教师的目标技能或领域知识，从而获得类似的能力。该过程涉及迭代调整学生模型的参数，以减少其输出与教师模型输出之间的差异，确保有效知识转移。本质上，上述四个阶段可以抽象为两个公式。第一个公式表示知识提取过程：D(kd) I={Parse( o, s)|o∼pT(o|I⊕s), ∀s∼ S} (1)，其中⊕表示合并两段文本，I表示任务、技能或领域的指令或模板以引导LLM并提取知识，s∼ S表示种子知识的一个示例，LLM可以基于此探索生成新知识，Parse( o, s) 表示解析浓缩。\n",
      "根据第2.4节的内容，知识蒸馏过程被分为两个主要步骤。“知识”部分专注于从教师LLM中提取知识（公式1），而“蒸馏”部分则专注于将这些知识注入学生模型中（公式2）。\n",
      "在后续部分我们将详细阐述这些过程。3.1 知识\n",
      "本节主要介绍从教师LLM中提取知识的方法。根据获取知识的方式，我们将它们分为标注（Labeling）、扩展（Expansion）、数据整理（Data Curation）、特征提取（Feature）、反馈（Feedback）和自我知识（Self-Knowledge）几类。图5展示了这些知识提取方法的示意图。\n",
      "\n",
      "3.1.1 标注\n",
      "标注知识指的是利用教师LLM对给定输入x的输出y进行标记，作为种子知识，根据指令I或示范c（c=（x1, y1），...，（xn, yn））。从教师LLM中提取知识的方法简单有效，已在多种任务和应用中广泛使用。它仅需要收集输入数据集并将其喂入LLM以获得所需生成内容。此外，通过预先定义的I和c，可以控制y的生成。该过程可以这样形式化表示：\n",
      "D(lab)={x, y|x∼X, y∼Y}\n",
      "y∼pT(y|I⊕c⊕x)}. (3) 输入x可以来自现有的NLP任务数据集，这些数据集为知识蒸馏提供了典型的储备。许多工作旨在利用强大的LLM作为标注器，为各种任务标注数据集样本。例如，在自然语言理解任务中，使用LLM对文本进行分类（Gilardi等，2023年；Ding等，2023a年；He等，2023a年），而在自然语言生成任务中，LLM协助生成输出序列（Hsieh等，2023年；Jung等，2023年；Wang等，2021b年）。文本生成评估任务利用LLM对评估结果进行标注（Li等，2024b年；Wang等，2023b年），推理任务则利用LLM标注Chain of Thought（CoT）解释（Hsieh等，2023年；Li等，2022年；Ho等，2023年；Magister等，2023年；Fu等，2023年；Ramnath等，2023年；Li等，2023d年；Liu等，2023g年），等等。而不是集中在特定任务上，许多工作并未如此。\n",
      "目前的工作重点是根据指令对输出进行标记，从而教会学生模型通过遵循指令来更灵活地完成任务。各种NLP任务的集合，结合指令模板，为x提供了宝贵输入。例如，FLAN-v2系列（Longpre等人，2023年）提供了大量的任务集合及其指令，这些任务的标签是由强大的LLMs生成的，如Orca（Mukherjee等人，2023年；Mitra等人，2023年）生成的。这些NLP任务的指令来自预定义的模板，缺乏多样性，并且可能与人类自然提问存在差距。人类与聊天模型之间的实际对话提供了大规模数据，其中包含由强大的LLMs标注的真实查询和生成内容，如ShareGPT。此外，Xu等人（2023b）和Anand等人（2023）从论坛如Quora和Stack Overflow中采样的实际问题进行标记。此外，标记过程还可以通过引导进行。\n",
      "起始文本中并未包含需要处理或删除的信息。直接提供处理后的文本即可。\n",
      "这些方法利用生成模型的上下文学习能力生成类似提供的示例数据。与标注方法不同，上下文方法中输入 \\(x\\) 和输出 \\(y\\) 都由教师生成模型生成。这个过程可以表示为：\\[D(exp) ={(x, y)|x∼p_T(x|I⊕c), y∼p_T(y|I⊕x)}\\] 其中，上下文 \\(c\\)、输入 \\(I\\) 和上下文 \\(I⊕c\\) 确保生成的数据符合上下文。上下文方法通过上下文学习能力生成新的数据样本，而不仅仅是进行数据标注。\n",
      "在不同教师语言模型（LLMs）的方法中，知识提取主要有以下几种方式：\n",
      "\n",
      "1. **Labeling**：教师根据输入生成输出。\n",
      "2. **Expansion**：通过上下文学习，教师生成与给定示范相似的样本。\n",
      "3. **Data Curation**：根据元信息（如主题或实体），教师合成数据。\n",
      "4. **Feedback**：教师提供对学生生成内容的反馈，包括偏好、修正和挑战样本的扩展等。\n",
      "5. **Self-Knowledge**：学生先生成输出，再通过学生自身过滤或评估以获取高质量内容。\n",
      "x和y代表根据教师LLM生成的新输入-输出对。输入x基于一组输入-输出演示c生成。输出y在指令I的指导下，根据新的输入x生成。注意到演示可以预先定义或通过添加新生成的样本动态更新。扩展技术已被广泛应用于从教师LLM中提取丰富的指令遵循知识。王等人（2022a）首先引入了迭代式抽样方法，自入教，利用LLM基于几组从175个手动编写指令中抽取的演示来生成多种指令。然后将新生成的指令添加回初始池，以受益于后续扩展迭代。随后，塔里奥等人（2023）将这种方法应用于更强大的教师LLM text-davinci-003，提取了52K高质量数据。为了提升\n",
      "在此扩展现有的任务数据集方面，Wu等人（2023c）和Sun等人（2024b）促使教师LLM生成对应特定主题的指令。Xu等人（2023a）提出了一种Evol-Instruct方法，通过两个维度扩展指令：难度（例如将问题重写得更复杂）和多样性（例如生成更多的末梢指令）。此Evol-Instruct方法是跨领域的，并且已被用于扩展编码（Luo等人，2023a）和数学（Luo等人，2023b）的知识蒸馏。此外，扩展现有的数据集方法可以显著增强类似样本的NLP任务数据集，从而提升任务性能。例如，AugGPT（Dai等人，2023a）利用教师LLM将训练样本中的每个句子重新表述为多个在概念上相似但语义不同的样本，以改善分类性能。类似地，TDG（He等人，2023b）提出了一种目标数据生成（TDG）框架。\n",
      "自动识别数据中的挑战性子群体，并通过上下文学习使用LLMs生成新的样本。总结来说，扩展方法利用LLMs的上下文学习优势，产生更加多样化和扩展性的数据集，既包括输入也包括输出。然而，生成数据的质量和多样性很大程度上依赖于教师的LLMs以及初始种子演示。这种依赖性可能导致数据中固有的偏差（Yu等，2023a；Wei等，2023）以及最终生成内容可能趋于相似的问题，从而限制该方法旨在实现的多样性（Ding等，2023b）。此外，扩展过程可能会无意中放大种子数据中存在的偏差。\n",
      "针对 Labeling 和 Expansion 方法存在的局限性，这些方法往往产生的数据质量不一且数量有限。在 Labeling 中，种子知识来自任务数据集，可能会引入噪声和脏数据。而在 Expansion 中，由种子演示生成的数据在大量生成时可能变得同质。为克服这些挑战，Data Curation 方法通过深入的元信息来确保高质或大规模数据的质量（Ding 等，2023b；Gunasekar 等，2023；Li 等，2023a；Mar，2023；Liu 等，2023d；Wei 等，2023；Yu 等，2024；Ye 等，2022；Gao 等，2023a；Yang 和 Nicolai，2023）。 Data Curation 的独特之处在于它从零开始合成数据。通过将诸如主题或知识点等多样化的元信息纳入这一过程，可以生成可控的 x 和 y。\n",
      "处理可以 meticulously 控制，以产生大规模且高质量的 数据集。数据整理的公式可以表示为：D(cur)={(x, y)|x∼pT(x|I⊕m), y∼pT(y|I⊕x)}。(5) 在这个公式中，m代表用于指导 x、y 合成的多样化元信息，I是指导教师 LLMs 生成 x 或 y 的指令。不同研究在元信息的来源和利用方法上有所不同。UltraChat（Ding等人，2023b）有效展示了通过知识蒸馏过程，同时整理出高质量且多样化的数据。他们从三个领域收集了广泛的元信息：世界问题、创作与生成以及现有材料的辅助。例如，在世界问题领域，他们探索了诸如“Technology”和“Food and Drink”等 30 个元主题。随后，教师 LLMs 会利用这些元信息提炼出多样化的指令和对话，实现了高质量且多样化的数据整理。\n",
      "UltraChat 在实例数量上超过 150 万个。UltraLLaMA 模型通过这个数据集进行微调后，表现始终超越其他开源模型。另一个值得注意的系列，即 phi(Gunasekar et al., 2023; Li et al., 2023a; Mar, 2023)，专注于从较小的质量较高的数据集中提取数据，类似于“教科书”内容。phi-1 (Gunasekar et al., 2023) 系统尝试在编码领域从语言模型中提取具有“教科书质量”的内容。他们的方法通过引导随机主题或函数名称来增强多样性，从 LLM 中提取清晰、完整的、具有指导性的内容。提取的数据包括含有自然语言解释和代码片段的 1 亿个 Python 教科书令牌，以及 1.8 亿个带解决方案的 Python 练习题令牌。令人惊讶的是，尽管模型较小，但在 HumanEval 和 MBPP 等编码基准上 phi-1 模型的表现甚至超过了几乎所有开源模型。\n",
      "模型规模变小了100倍，而数据集规模则缩小了10000倍。MFTCoder（刘等人，2023d）通过数百个Python知识点作为元信息创建了一个CodeExercise数据集。相比之下，Magicoder（韦等人，2023）和WaveCoder（余等人，2024）从开源代码数据集中获取原始代码集合，将其用作元信息来生成教学数据。在自然语言处理（NLU）任务中，某些研究（叶等人，2022；高等人，2023a；王等人，2021a）探索使用标签作为元信息来合成样本，从而实现数据增强。类似地，在信息检索任务中，也有利用文档作为元信息来生成潜在查询，从而构建大规模检索对（邦弗拉西奥等人，2022；孟等人，2023）。总之，通过教师语言模型（LLM）进行数据治理已经成为一种有前景的技术，能够合成不仅高质量多样性的数据集。\n",
      "大规模的成功表明这种方法的有效性。在特定领域取得成功的模型如phi-1证明了这一点。创建合成数据集将成为AI领域的一项重要技术技能和重点。白-box知识蒸馏提供了一种更透明且易于访问的研究方法。它利用教师大模型的输出分布、中间特征或激活来进行蒸馏，我们将这些知识统称为‘Feature知识’。此前研究主要关注较小的基于编码器的大模型，这些模型通常参数少于1亿（详情见Gou等人（2021））。然而，近期的研究已经开始探索基于生成模型的大模型中的白-box蒸馏方法。\n",
      "文本处理完成如下：\n",
      "\n",
      "2023年，Liang等；2023年，Gu等；2024年，Agarwal等；2023年，Liu等；2023年，Wen等；2024年，Wan等；2023年，Zhao和 Zhu；2023年，Qin等；2024年，Boizard等；2024年，Zhong等）。获取此类特征知识的典型方法是让教师的预训练语言模型（teacher LLMs）在输出序列y的基础上，标注出其内部表示。然后，这些标注通过诸如Kullback-Leibler散度（KLD）等方法进行提炼。特征知识的提取过程可以表示为：D(feat)={(x, y, ϕ feat(x, y;θT))|x∼ X, y∼ Y} 。在这个表达式中，Y是输出集合，可以由教师的预训练语言模型生成，也可以由学生模型生成，或者是直接从数据集中获取的。ϕfeat(·;θT)表示从教师的预训练语言模型中提取特征知识（如输出分布）的操作。最为直接的方法是从固定序列数据集中标记一个带有令牌级概率的固定数据集。\n",
      "多个研究探讨了如何利用教师模型的中间层来提取丰富的语义和语法知识。Sanh等人（2019）和Wen等人（2023）的研究表明了这一点。Liang等人（2023a）提出的TED方法设计了一种任务相关的逐层蒸馏方式。他们令学生模型在每一层的隐藏表示与教师模型对齐，从而有选择地提取与目标任务相关的知识。Gu等人（2024）和Agarwal等人（2024）引入了一种新颖的方法，即让学生模型首先生成序列，称为“自动生成序列”。学生模型通过使用教师模型输出分布的反馈来学习。这种方法尤其适用于学生模型缺乏模仿教师模型分布能力的情况。此外，Tao等人（2022a）、Liu等人（2023a）和Kim等人（2023b）提出了多种基于蒸馏教师模型特征知识的LLM量化方法。这些方法旨在在量化过程中保留原始输出分布。\n",
      "量化语言模型（LLMs）时，确保性能损失最小化。此外，特征知识可以作为多教师知识蒸馏的强大来源。Timiryasov和Tastet（2023）通过将GPT-2和LLaMA的多个模型作为教师模型来提取输出分布。类似地，Wan等人（2024a）通过将各种LLMs的能力通过它们输出分布的加权融合整合到一个单一的LLM中，实现了创新。这种方法有可能显著提升学生模型的能力，超越任何单一教师LLM。总之，使用特征知识提供了一种更透明的替代方法，允许对蒸馏过程有更深的洞察和控制。通过利用教师LLM的特征，如输出分布和中间层特征，白盒方法实现了更丰富的知识转移。尽管展示了潜力，尤其是在\n",
      "对于较小的模型，其应用不适合黑箱语言模型（LLM），在这种模型中，内部参数是不可访问的。此外，来自白箱语言模型的模型从黑箱教师模型（例如GPT-4）中提取时，可能会表现不佳，因为黑箱教师模型往往更强大。3.1.5 反馈 最早的工作大多仅强调从教师到学生的单向知识转移，不考虑从教师对学生的生成进行反馈。教师对学生的生成通常通过提供偏好、评估或纠正信息来提供指导。例如，一种常见的反馈形式是教师对学生的生成进行排名，并通过强化学习自适应反馈（RLAIF）将其偏好整合到学生模型中（Bai等人，2022a）。这是一个用于提取反馈知识的一般化公式： D(fb)={(x, y, ϕ fb(x, y;θT))|x∼ X, y∼pS(y|x)} (7)\n",
      "这里的 \\( y \\) 表示对学生模型对 \\( x \\) 的响应生成的输出。\\( \\phi_{fb}(\\cdot;\\theta_{T}) \\) 表示提供来自教师语言模型的反馈。这一操作通过提供评估、纠正信息或其他形式的指导来评估学生的输出 \\( y \\)，给定输入 \\( x \\)。这种反馈知识不仅可以提炼为学生，还可以生成反馈（例如创建学生偏好模型），更重要的是，它能够帮助学生根据反馈来改进他们的回应。已经探索了多种方法来提取这种高级知识（Bai等人，2022a; 洛等人，2023b; 崔等人，2023a; 元等人，2023; 江等人，2023b; 陈等人，2023a; 顾等人，2024; 阿戈瓦尔等人，2024; 陈等人，2024b; 郭等人，2024; 盛等人，2023; 李等人，2023a）。偏好，如之前讨论的，是从教师模型中获得的一种显著的反馈知识形式。可以从各种偏好知识中提炼出不同的知识。\n",
      "文本处理完成如下：\n",
      "\n",
      "- Bai et al. (2022a) 提出了一种从 LLM 中提炼危害性偏好（harmlessness preferences）的方法，称为 RLAIF。他们使用经过 SFT（Self-Supervised Fine-Tuning）训练的 LLM 来生成每个提示的响应对，然后根据危害性对这些响应对进行排序，创建一个偏好数据集。这个数据集通过提炼（distillation）转换成一个偏好模型（Preference Model, PM），然后引导更无害的 LLM 政策的 RL（Reinforcement Learning）训练。\n",
      "- 在数学推理方面，Wizard-Math（Luo et al., 2023b）做了强调。他们使用 ChatGPT 作为老师，直接提供过程监督，并评估生成解决方案中的每一步正确性。\n",
      "- 为了扩大高质量提炼的偏好数据集，Cui 等（2023a）开发了一个大规模的偏好数据集，称为 UltraFeedback。他们收集了各种指令和模型，以产生比较数据。之后，GPT-4 被用来从多个方面评估候选者，包括指令遵循、真实性、诚实性。\n",
      "教师不仅可以评估学生的生成，还可以提供详细的反馈，特别是在学生表现不佳的情况下。例如，Lion (Jiang et al., 2023b) 中，教师模型指出学生模型遇到的挑战性指令，并生成更具挑战性的新指令以提高学生的技能。PERsD (Chen et al., 2023a) 展示了一种方法，即教师根据学生生成的错误代码片段中遇到的具体执行错误提供定制化改进反馈。类似地，SelFee (Ye et al., 2023) 利用ChatGPT生成反馈并根据反馈修订学生的答案。相反地，FIGA (Guo et al., 2024) 是通过将学生的响应与真实响应进行比较来修订学生的响应。此外，教师模型对学生生成的分布本身也可以作为一种反馈形式。MiniLLM (Gu et al., 2024) 和GKD (Agarwal et al., 2024) 提出了一种创新策略。\n",
      "在该方法中，学生模型最初生成序列，随后教师模型生成一个输出分布作为反馈。这种方法利用教师的洞察直接指导和优化学生模型的学习过程。3.1.6 自我知识 该知识也可以从学生自身提取，我们将其称为自我知识。在这种设置下，同一个模型既是教师也是学生，通过提炼和优化自己之前生成的输出，迭代地自我改进。这种知识可以绕过需要外部的、可能具有专有性质的强大教师模型，如GPT系列的大型语言模型（LLMs）。此外，它允许模型超越传统教师-学生的局限性或“天花板”。自我知识可以表示为： \\(D(sk) = \\{(x, y, ϕ_{sk}(x, y))|x \\sim S, y \\sim p_S(y|I \\oplus x)}\\), 式中，\\(ϕ_{sk}(\\cdot)\\) 是一个表示额外过程的通用函数，用于自动生成输出 \\(y\\)。\n",
      "该机制可能包括但不限于过滤、奖励或其他方法来提升或评估 y。它可能由外部工具或学生自身 θS 管控。近年来，该领域的研究提出了多种创新方法来激发自我认知，展示了其创建更高效、自主学习系统的潜力。（Allen-Zhu 和 Li, 2020; Wang 等人, 2022a; Sun 等人, 2024b; Yang 等人, 2024; Jung 等人, 2023; Huang 等人, 2023a; Gulcehre 等人, 2023; Yuan 等人, 2024a; Xu 等人, 2023b; Zelikman 等人, 2022; Chen 等人, 2024a; Zheng 等人, 2024; Li 等人, 2024c; Zhao 等人, 2024; Singh 等人, 2023; Chen 等人, 2024c; Hosseini 等人, 2024）一个例子就是 Self-Instruct（Wang 等人, 2022a），它利用 GPT-3 通过扩展方法进行数据增强，生成额外的数据样本以增强数据集。随后，该数据集会微调原始模型。\n",
      "一些方法通过修改提示来诱使其生成目标知识。例如，在 Self-Align (Sun et al., 2024b) 中，他们发现使用 Self-Instruct 数据微调的模型倾向于生成简短或间接的回复。他们给这个模型以详尽的指令来生成深入且详细的回复。然后，他们使用上下文蒸馏（Askell et al., 2021）将这些回复与非详尽的指令配对，重新蒸馏回模型。类似地，RLCD (Yang et al., 2024) 引入对比提示来从不一致的 LLM 中生成偏好对，涵盖优劣示例。一个基于这些对训练的偏好模型则通过强化学习引导不一致模型的提升。其他一些方法通过过滤方法来精制自动生成的数据。例如，Impossible Distillation (Jung et al., 2023) 目标是句子。\n",
      "处理后的文本：\n",
      "处理后的文本：总结任务主要通过基于 entailment、长度和多样性的过滤器来筛选自动生成的总结。LMSI（黄等人，2023a）为每个问题生成多条 CoT（Continue to Think）推理路径和答案，然后仅保留那些路径中能够得出最一致答案的路径。需要注意的是，随着学生模型的持续改进，可以迭代地获得精进的知识。这类似于 Gulcehre等人（2023）介绍的强化自训练（ReST）框架，它通过交替进行Grow和Improve阶段来逐步获得更好的自知识，并进一步提升学生的技能。在Grow阶段，学生模型生成多个输出预测；随后在Improve阶段，通过使用评分函数对这些自生成输出进行排序和过滤。最后，语言模型会在经过精心筛选的这个数据集上进行微调，采用离线强化学习目标。自玩游戏（陈等人，2023）则展示了这一过程。\n",
      "2024年的一项框架，类似于迭代DPO，其中语言模型被微调以区分自动生成的响应和人工标注数据。这些自动生成的响应可被视为“负面知识”，以帮助学生更好地与目标分布对齐。自奖励（Yuan等人，2024年）通过利用语言模型本身作为奖励模型，探索了一种新颖且有前途的方法。它使用LLM作为裁判提示，自主为自动生成的响应分配奖励。整个过程可以迭代进行，从而提升指令遵循和奖励建模能力。3.2 学徒化 本节探讨了有效从教师LLM中转移提取知识的方法，将其转移到学生模型中。我们探索了一系列的学徒化技术，包括受监督微调策略、差异与相似性策略，以及先进的方法，如强化学习。\n",
      "在监督微调（SFT）或称为序列级别知识蒸馏（SeqKD）（Kim和Rush，2016）方法中，学生模型通过最大化生成的序列似然性来与教师模型的预测对齐。这种过程在数学上可以表示为最大化学生模型生成序列的概率。\n",
      "形式化为最小化目标函数: LSFT=Ex∼X,y∼pT(y|x)[−logpS(y|x)]，(9) 其中y是教师模型生成的输出序列产生的序列。这种简单而高效的技巧构成了领域内众多研究的基础。许多研究人员成功地使用SFT训练学生模型，利用由教师LLM生成的序列（Taori等人，2023；Chiang等人，2023；Wu等人，2023c；Xu等人，2023a；Luo等人，2023b）。此外，SFT已在许多自编码工作中被探索（Wang等人，2022a；Huang等人，2023c；Xu等人，2023b；Zelikman等人，2022）。由于大量使用SFT的迁移学习工作，这里仅列出代表性的例子。更多详细工作可以在第4.3.2.2节中找到。\n",
      "groups 包括那些致力于减少概率分布差异的群体和那些旨在增强隐藏状态相似性的群体。减少差异。基于差异的方法通过最小化教师模型和学生模型概率分布之间的差异来减少差异，差异函数为 D: LDiv= E x∼X,y∼Y[D(pT(y|x), pS(y|x))], (10) 差异函数 D 的具体形式取决于所使用的差异类型。表 1 列出了不同差异测量方法下 D 的功能形式。常用的 KL 效果本质上是在近似目标分布时最小化前向 KL 散度 (Sanh 等人，2019; 12 篇论文)。KL 散度最小化 p||q 的 argminqKL(argsminqKL(q||p)) Fig. 6: 目标分布逼近中前向和反向 KL 散度的比较。前向 KL 散度方法倾向于覆盖目标分布的所有模式，但精度较低，即“模式覆盖”行为。反向 KL 散度方法侧重于专注于。\n",
      "论文中提到，通过迫使学生模型覆盖所有教师模型的模式（predominantly on the most prominent mode），学生模型表现出“模式寻求”行为（Wen et al., 2023; Timiryasov and Tastet, 2023; Liang et al., 2023a; Chen et al., 2024d）。这种行为迫使学生模型覆盖所有教师模型的模式，但当学生模型无法学习高复杂度教师模型的所有模式时，可能会导致学生模型在教师的分布下给概率较低的令牌分配较大的概率质量。这可能导致潜在的幻觉和低质量生成。或者，通过使用反KL散度（reverse KL divergence），可以优先考虑教师模型中分配高概率的令牌，从而减轻低质量输出的风险，但也可能牺牲多样性。Gu等（2024）采用反KL散度来防止学生模型过度估计。\n",
      "Policy Gradient方法被用于优化低概率区域教师分布。Agarwal等人（2024）和Sason及Verdú（2016）评估了不同差异函数在LLM迁移学习中的效果，发现最优差异函数取决于任务性质。例如，在机器翻译任务中，前向KL差异更适合，因为输出模式较少。而在对话生成和指令调优任务中，后向KL差异效果更好，因为这些任务涉及多个模式和更广泛的可能性响应。因此，任务性质显著影响了差异函数的选择，以实现最佳性能。类似地，知识迁移学习中的相似性方法旨在使学生模型的隐藏状态或特征与教师模型的隐藏状态或特征对齐。这些方法使用各种相似性指标来测量并优化内部表示之间的相关性。\n",
      "为了确保学生模型不仅在输出上与教师模型相似，还在处理信息的方式上也相似，我们使用如下公式来定义相似性目标：\\[LSim=E_{x∼X,y∼Y}[LF(\\Phi_T(f_T(x, y)),\\Phi_S(f_S(x, y)))],\\]其中，\\(f_T(x, y)\\)和\\(f_S(x, y)\\)分别是教师模型和学生模型的特征图。通过应用变换函数\\(\\Phi_T\\)和\\(\\Phi_S\\)使得这些特征图具有相同的形状，便于直接比较。相似函数\\(LF\\)用于匹配这些变换后的特征图。表2展示了\\(LF\\)的常见选择。目前，较少的工作在大型语言模型（LLMs）的知识蒸馏（KD）中使用基于相似性的方法。其中，Liang等人（2023a）提出了一种任务意识的层次式蒸馏（TED），该方法利用任务意识的滤波器。这些滤波器被设计为从教师模型中选择性地捕捉特定任务最相关的信息。核心目标是最小化这些滤波器的差异。\n",
      "关于知识蒸馏，基于增强学习的方法在编码器型语言模型（Sun et al., 2019, 2020; Jiao et al., 2020; Hou et al., 2020; Zuo et al., 2022; Liang et al., 2021）中已有广泛应用，但在语言模型（LLM）的知识蒸馏中则较少见。然而，考虑到其有效性，我们预计未来的研究将探索这些方法在知识蒸馏中的应用。3.2.3 增强学习 本节探讨了利用增强学习（RL）方法将知识注入学生模型的高级方法。这种方法尤其适用于利用教师模型的反馈来训练学生模型（Bai et al., 2022a; Cui et al., 2023a; Luo et al., 2023b; Agarwal et al., 2024; Chen et al., 2024b; Ma et al., 2023a; Pang et al., 2023; Du et al., 2023a）。基于增强学习的知识蒸馏过程通常包括两个主要阶段：蒸馏奖励模型训练。第一阶段\n",
      "本文涉及训练奖励模型 \\( \\varphi \\)，使用教师语言模型生成的反馈数据 \\( D(f_d) \\) 来进行训练。偏好数据作为典型的反馈方式被用于训练学生奖励模型（Bai等人，2022a；Cui等人，2023a；Lee等人，2023a；Kim等人，2023a）。它们通常由输入输出对（\\( x, y_w, y_l \\)）组成，其中 \\( y_w \\) 和 \\( y_l \\) 分别表示相对于教师偏好而言的“赢得”和“输掉”输出。奖励模型的损失函数被定义为： \\[\n",
      "LRM(\\varphi, D(f_d)) = - \\mathbb{E}_{(x, y_w, y_l) \\sim D(f_d)}[\\log \\sigma(\\varphi(x, y_w) - \\varphi(x, y_l))] \\quad (12)\n",
      "\\] 这种形式指导奖励模型根据教师的标准正确区分更优和劣质的输出。RLMEC（Chen等人，2024b）则采用不同的方法，通过训练一个生成奖励模型来代替学习实例级别的奖励。它从教师语言模型中提取错误解决方案重写数据，并进行训练以产生词级奖励，用于强化学习。\n",
      "在第二阶段，学生模型通过策略πθ进行优化，以最大化期望奖励，并同时最小化与参考策略πref的差异，通常是指学生模型的初始策略通过SFT训练得到的策略。这个差异由因子β控制。RL目标表达为：最大化下式中的期望值：\\[ \\max_{\\pi_{\\theta}} E_{x \\sim X, y \\sim \\pi_{\\theta}(y|x)}[r_{\\phi}(x, y)] - \\beta \\text{DKL}[\\pi_{\\theta}(y|x) \\| \\pi_{ref}(y|x)] \\] 式中，\\(r_{\\phi}(x, y)\\)是奖励模型得到的奖励。当使用PPO（Schulman et al., 2017）算法时，RL提供了一个有效机制，使得学生模型的输出能够与教师保持一致。此外，教师的LLM也可以作为奖励模型直接在RL过程中分配奖励，从而避免训练奖励模型的需要（Lee et al., 2023a; Kwon et al., 2023）。这种做法可以显著提升效果。\n",
      "在进行排名优化时，相比于使用较小的精馏奖励模型，这会带来更高的计算成本。3.2.4 排序优化 排序优化为在语言模型中注入偏好反馈提供了一个稳定且计算效率高的替代方案（Rafailov 等人，2023；Song 等人，2023a；Yuan 等人，2023b）。这种方法与传统的 RL 方法不同，它在微调期间直接将排名信息纳入语言模型中。直观上，它直接更新策略以增加偏好响应的相对可能性。无需进行输出采样，这种直接优化偏好使过程更加稳定且高效。最近，有一些工作探索使用排序优化将教师的偏好转移到学生模型中（Tunstall 等人，2023；Hong 等人，2023；Yuan 等人，2024a）。Zephyr（Tunstall 等人，2023）使用直接排序优化。\n",
      "Preference Optimization (DPO) (Rafailov et al., 2023)用于提取教师语言模型的偏好一致性。DPO将强化学习（如公式13所示）中基于奖励最大化的KL散度约束的目标简化为单阶段策略训练。具体来说，DPO的目标是最大化以下期望值： $$E_{x,y,w,l}\\sim D(f_d) \\log \\sigma^{\\beta \\log \\pi^{\\theta}(y_w|x) \\pi_{ref}(y_w|x) - \\beta \\log \\pi^{\\theta}(y_l|x) \\pi_{ref}(y_l|x)}$$ 其中，\\(yw\\) 按照教师语言模型的偏好优于 \\(yl\\)。Hong等（2023）（Hong et al., 2023）采用两种基于排名的优化目标，即Align Human Feedback通过响应排名优化（RRHF）（Yuan et al., 2023b）和偏好排名优化（PRO）（Song et al., 2023a），用于偏好提取。RRHF（Yuan et al., 2023b）定义了一个排名损失，公式为： $$L_{RRHF} = \\sum_{i < j} \\max(0, p_i - p_j)$$ 其中，\\(p_i\\) 和 \\(p_j\\) 分别是教师语言模型为响应 \\(y_i\\) 和 \\(y_j\\) 分配的奖励分数，\\(ri\\) 和 \\(rj\\) 分别是 \\(y_i\\) 和 \\(y_j\\) 的排名。\n",
      "在基于策略πθ的情况下，此方法强调直接基于教师偏好比较和排序响应。PRO（Song等人，2023a）扩展了成对比较的概念，以处理任意长度的偏好排序。给定指令x和按照教师偏好顺序排列的响应序列y1≻y2≻...≻yn，RPO训练目标为： \\( L_{PRO} = -\\sum_{k=1}^{n-1} \\log \\left( \\frac{\\exp(p_k)}{\\sum_{i=k}^{n} \\exp(p_i)} \\right) \\) （16） 式中，\\(p_k\\) 表示在学生策略πθ下，y_k的条件Log概率。通过迭代对比生成响应的似然性，PRO优化学生LM，使其优先考虑最被青睐的响应，并逐步按偏好递减顺序排名其余响应。4 S KILL DISTILLATION 基于第3节关于获取知识和蒸馏算法的概述，我们将注意力转移到如何利用这些技术在大型语言模型（LLMs）中提炼特定技能。我们的探索将涵盖多种方法。\n",
      "LMs表现出的技能范围包括：Context Following（理解并有效回应输入信息的能力）、Alignment（确保输出与教师响应一致的能力）、Agent（自主性质）、NLP Task Specialization（在自然语言处理任务上的专精能力）以及Multi-Modality（从教师LMs中转移知识到多模态模型）。Context Following着重于学生的理解和回应输入信息的能力。Alignment则关注学生的输出能够与教师的响应保持一致。接下来，Agent强调了LMs的自主性质。NLP Task Specialization展示了LMs在多种自然语言处理任务上的适应性。最后，Multi-Modality涵盖了从教师LMs中转移知识到多模态模型的知识。表3总结了代表性的作品，包含了涉及的技能、种子知识、教师LMs、学生模型、知识提取方法和训练目标。4.1部分专注于从LMs中提取Context Following的能力。这一过程包括。\n",
      "通过将LLMs的处理复杂情境的能力——例如少量示范、复杂的指示、对话历史和检索增强信息——转移到较小的模型中，从而将处理复杂情境的能力转移到较小的模型中。该领域中的许多研究努力旨在赋予较小模型这些复杂的、基于情境的能力。我们在此讨论将这些能力转移到较小模型中的方面，根据不同的情境类型进行分类，并详细说明如何对其进行提炼和融入。\n",
      "以下是处理后的文本内容：\n",
      "\n",
      "- 这里罗列了多种方法来实现知识提取，包括自监督学习（SFT）和自知学习（Self-Knowledge）等方法。\n",
      "- 主要包括了几项研究和项目，如：\n",
      "  - Wang等人（2022a）的14种方法。\n",
      "  - Taori等人（2023）的175种自定义任务。\n",
      "  - Wu等人（2023c）的3.5K类别和混合数据集。\n",
      "  - Jiang等人（2023b）的10M单词和小型LLaMA。\n",
      "  - Sun等人（2024b）的人工编写原则和小型LLaMA。\n",
      "这些研究的列表涵盖了多种方法和技术，用于改进语言模型的自我奖励学习。以下是整理后的关键信息：\n",
      "\n",
      "1. **Self-Rewarding (Yuan et al., 2024a)**\n",
      "   - 使用人类编写样本进行训练。\n",
      "\n",
      "2. **Self-Knowledge SFT + RL (Zelikman et al., 2022)**\n",
      "   - 结合算术、常识问答和GSM8K数据集进行自我增强学习。\n",
      "\n",
      "3. **GPT-J GPT-J Self-Knowledge SFT**\n",
      "   - 使用GPT-J进行自我知识增强学习。\n",
      "\n",
      "4. **Alpaca Dataset GPT4 LLaMA Labeling SFT**\n",
      "   - 使用Alpaca数据集和GPT4进行标记学习。\n",
      "\n",
      "5. **Selective Reflection-Tuning (Li et al., 2024d)**\n",
      "   - 使用Alpaca/WizardLM数据集进行选择性回射调优。\n",
      "\n",
      "6. **Koala (Geng et al., 2023)**\n",
      "   - 使用人类对话数据进行标记学习。\n",
      "\n",
      "7. **Baize (Xu et al., 2023b)**\n",
      "   - 使用Quora和Stack Overflow数据进行标记学习。\n",
      "\n",
      "8. **UltraChat (Ding et al., 2023b)**\n",
      "   - 使用Wikidata、文本材料和C4数据进行标记学习。\n",
      "\n",
      "9. **Orca (Mukherjee et al., 2023)**\n",
      "   - 使用Flan-v2和GPT4进行标记学习。\n",
      "\n",
      "10. **Orca2 (Mitra et al., 2023)**\n",
      "    - 使用Flan-v2、少量样本、数学和合成数据进行标记学习。\n",
      "\n",
      "这些方法和技术在标记学习和自我增强学习方面进行了探索，以提升语言模型的性能。\n",
      "处理后的文本：\n",
      "Hsieh et al., 2023 人类对话模型 ChatGPT LLaMA 标注 SFT\n",
      "Zhang et al., 2023a QA 数据 ChatGPT 聊天GLM 聊天模型 Vicuna-7B LLaMA 标注 SFT\n",
      "Li et al., 2024e 争议性话题聊天GPT LLaMA 标注 SFT\n",
      "Gunasekar et al., 2023 GPT3.5 phi-1 编集 SFT phi-1\n",
      "Li et al., 2023a GPT3.5 phi-1 编集+标注 SFT\n",
      "Luo et al., 2023c 阿尔帕卡数据GPT4 LLaMA 标注 SFT\n",
      "Kang et al., 2023b 开放指令GPT4 LLaMA 标注 SFT\n",
      "Wang et al., 2023c 偏好对话模型GPT4 LLaMA 标注 SFT\n",
      "Tunstall et al., 2023 多重数据集GPT4 Mistral 标注 SFT\n",
      "处理后的文本如下：\n",
      "\n",
      "- LLaMA Expansion + Labeling SFT + RL RLCD (Yang et al., 2024)\n",
      "- PaLM 2 Labeling + Feedback RL GPT3 Reward (Kwon et al., 2023)\n",
      "- GPT3 Labeling RL ILF (Scheurer et al., 2023)\n",
      "- LLaMA Labeling RL Constitutional AI (Bai et al., 2022a)\n",
      "- GPT4 LLaMA Labeling RL SANDBOX (Liu et al., 2023b)\n",
      "- ChatGPTLLaMA Data Curation SFT + RL Agent Toolformer (Schick et al., 2023)\n",
      "- ChatGPT GPT-J + LLaMA Labeling SFT Gorilla (Patil et al., 2023)\n",
      "这里的文本主要是关于不同工具和方法的介绍，看起来是一份API文档或工具介绍的汇总。以下是处理后的文本内容：\n",
      "\n",
      "- API Documentation GPT4 LLaMA Expansion SFT\n",
      "- GPT4Tools (Yang et al., 2023b) Tool\n",
      "- Image Content ChatGPT LLaMA Curation + Expansion SFT Tool\n",
      "- Alpaca (Tang et al., 2023a) Tool\n",
      "- Public-apis Repository\n",
      "- ChatGPT LLaMA Curation SFT Tool\n",
      "- LLM (Qin et al., 2023a) Tool\n",
      "- Real-world APIs\n",
      "- ChatGPT LLaMA Labeling SFT MLLM-Tool (Wang et al., 2024) Tool\n",
      "- HuggingFace Model Cards\n",
      "- GPT4 LLaMA Curation SFT\n",
      "- FireAct (Chen et al., 2023b) Planning\n",
      "- Mixed QA Dataset GPT4 LLaMA Labeling SFT\n",
      "- AgentTuning (Zeng et al., 2023a) Planning\n",
      "- 6 Agent Tasks\n",
      "- GPT4 + ChatGPT LLaMA Labeling + Expansion SFT\n",
      "- Lumos (Yin et al., 2023a) Planning\n",
      "- Mixed Interactive Tasks\n",
      "- GPT4 LLaMA Labeling SFT\n",
      "- AUTOACT (Qiao et al., 2024) Planning\n",
      "- Mixed QA Tasks\n",
      "- LLaMA LLaMA Labeling SFT\n",
      "- NLP Task Specialization AugGPT (Dai et al., 2023a) NLU\n",
      "- Amazon/Symptoms/PubMed20k Dataset\n",
      "- ChatGPT BERT Label SFT\n",
      "- TDG (He et al., 2023b) NLU\n",
      "- SST + QQP + MNLI GPT3 BERT Expansion SFT\n",
      "- SunGen (Gao et al., 2023a) NLU\n",
      "- Text Classification Tasks\n",
      "这里是文本处理后的版本：\n",
      "\n",
      "al., 2021a) NLU NLU Tasks GPT3 BERT Expansion SFT InheritSumm (Xu et al., 2023c) NLG Pile + ArXiv + CNN/DM + WikiHow GPT3.5 ZCode++ Label SFT DIMSUM+ (Jung et al., 2023) NLG None GPT2 + CTRL + BioGPT T5 Curation + Self-Knowledge SFT Genie (Yehudai et al., 2024) ELI5 + ASQA + NQ + CNN/DM Falcon + LLaMA FLAN + LLaMA Label SFT GKD (Agarwal et al., 2024) NLG/NLU/IF XSum+WMT14 en-de+GSM8K+FLAN2021 T5-XL T5 Feature + Feedback D&S + RL QUILL (Srinivasan et al., 2022) IR IR Datasets T5 RankVicuna (Pradeep et al., 2023a) IR IR Datasets ChatGPT LLaMA Labeling SFT RankZephyr (Pradeep et al., 2023b) IR IR Datasets ChatGPT + GPT4 Mistral Labeling SFT NDR (Mysore et al., 2023) Recommendation Recommendation Datasets GPT3 MPnet-110M Labeling SFT InstrcutRec (Zhang et al., 2023b) 39 instruction templates ChatGPT Flan-T5 Expansion + Self-Knowledge SFT ONCE (Liu et al., 2023c) Recommendation Recommendation Dataset ChatGPT LLaMA Labeling SFT\n",
      "Clean (Jain et al., 2023) Code Human-written Instructions LLaMA LLaMA Expansion + Self-Knowledge SFT Code\n",
      "这里是处理后的文本：\n",
      "2023年，代码代码数据集 ChatGPT LLaMA Labeling SFT Multi-Modality LLaVA (Liu et al., 2023) Vision-Language COCO GPT4 LLaMA Labeling SFT SVIT (Zhao et al., 2023b) Vision-Language Visual Genome + COCO GPT4 LLaMA Labeling SFT LVIS-Instruct4V (Wang et al., 2023e) Vision-Language LVIS GPT4V LLaMA Labeling SFT LLaVAR (Zhang et al., 2023d) Vision-Language LAION GPT4 LLaMA Labeling SFT Macaw-LLM (Lyu et al., 2023) 多模态 数据集 ChatGPT LLaMA Labeling SFT MIMIC-IT (Li et al., 2023f) 多模态 数据集 ChatGPT LLaMA Labeling SFT ChatBridge (Zhao et al., 2023d) 多模态 Task-Specific/Multimodal-Chat 数据 GPT4 + ChatGPT LLaMA Labeling SFT 表格 3：技能蒸馏工作的总结 IF: Instruction Following, MD: Multi-turn Dialoue, TP: Think Pattern, RAG: Retrieval-Augmented Generation, NLU: Natural Language Understanding, NLG: Natural Language Generation, IR: Information Retrieval, SFT: Supervised Fine-Tuning, D&S:\n",
      "Divergence and Similarity, RL: Reinforcement Learning, RO: Ranking Optimization. 这样的格式化方式有模板，比如给机器翻译数据预前缀为“Translate this sentence to Spanish:”。然而，这些方法存在局限性。手动数据生成工作量大，而基于模板的转换缺乏多样性的指令，并且可能无法很好地与自然的人类输入相匹配。像GPT-4这样的LLM提供了一个高效的方法来生成多样化的受控SFT数据，通过其上下文学习能力和指令遵循能力。目前，大多数相关工作使用OpenAI的GPT系列模型生成提示-响应数据对，然后通过监督微调来训练学生LLM（Wang et al., 2022a; Taori et al., 2023; Chiang et al., 2023; Wu et al., 2023c; Xu et al., 2023a; Mukherjee et al., 2023; Mitra et al., 2023; Luo et al., 2023b; Peng et al., 2023a）。Self-Instruct（Wang et al., 2022a）利用了GPT系列模型上下文学习的能力。\n",
      "GPT-3 将种子池扩展了 15 个任务，将其扩展为 52,000 个任务agnostic 指令，确保涵盖广泛的一系列通用指令。此外，引入了过滤和后处理阶段以消除冗余或相似的指令。通过使用此丰富数据集进行训练，GPT-3 获得了遵循指令的能力，使其在零样本指令任务中与 InstructGPT 相当，并在提供专家编写的新任务指令时也能表现良好。基于自指导方法，Taori 等人（2023）使用 Llama 7B 模型训练了一个 Alpaca 模型，其生成的 52,000 个指令遵循数据集模仿自指导方法，但利用了更强大的 text-davinci-003 模型。为了提高指令数据的多样性，Wu 等人（2023c）引入了一种称为 Topic-Guided Instruction Generation 的技术。该方法涉及从维基百科中收集 3.5K 个常见主题，用作生成过程中的指导。\n",
      "Complex Instructions. Some works promote students to solve more complex instructions (Xu et al., 2023a; Luo et al., 2023b,a; Guo et al., 2023c). According to Xu et al. (2023a), instruction datasets derived from human-written seeds often exhibit low to moderate complexity. To enhance the complex instruction-following capabilities of smaller models, WizardLM (Xu et al., 2023a) introduces Evol-Instruct. This method gradually transforms instructions into more complex forms through a multi-step evolution process, focusing on both increasing difficulty levels and expanding the diversity of topics. They conducted four rounds of evolution using the OpenAI ChatGPT API, resulting in a dataset of 250k complex instructions. Subsequently, they trained the LLaMA 7B model, referred to as WizardLM, on this dataset. In the high-difficulty section of test instructions, WizardLM even outperformed ChatGPT, achieving a win rate 7.9% higher than ChatGPT. Zhao et al. (2023e) further conducts.\n",
      "初步研究揭示了增加教学复杂度的有效性。Instruction Fusion（Guo等人，2023c）通过使用教师语言模型（Teacher LLMs）来增加复杂度，通过融合两个独立进化的指令。此外，将“进化”指令的概念扩展到了特定技能的提炼，例如编程（Luo等人，2023a）和数学（Luo等人，2023b）。相比之下，一些工作依赖于从ChatGPT生成指令，这些指令可能缺乏多样性，难以覆盖实际的人类指令。而Vicuna（Chiang等人，2023）和Koala（Geng等人，2023）则通过使用人类对话和社区贡献的自然指令展示了出色的性能。这些对话可以在像ShareGPT这样的平台上找到，用户可以在上面分享他们与ChatGPT的互动。值得注意的是，基于这些自然对话训练的模型可能会模仿风格，但未必能完全覆盖。\n",
      "capture the reasoning process of the original teacher (Gudibande et al., 2023; Mukherjee et al., 2023). System Instructions. To encourage student models to learn the reasoning process, Orca and Orca 2 (Mukherjee et al., 2023; Mitra et al., 2023) enhance the prompt and response data pairs by introducing a system message (e.g., \"explain like I’m five, think step-by-step\") to encourage student models to grasp the reasoning process. This system message prompts GPT-4 to provide explanation traces that elaborate on the teacher’s reasoning process. Orca 2 (Mitra et al., 2023) further trains the student model to identify the most effective solution strategy for each task, guided by Orca’s performance. This approach significantly improves the ability of smaller models to follow instructions involving reasoning. High-Quality Instructions. As demonstrated in Zhou et al. (2023a) and Li et al. (2024f), data quality is crucial for instruction following training. UltraChat (Ding et al., 2023) enhances the prompt and response data pairs by introducing a system message to encourage student models to grasp the reasoning process. This system message prompts GPT-4 to provide explanation traces that elaborate on the teacher’s reasoning process. Orca 2 further trains the student model to identify the most effective solution strategy for each task, guided by Orca’s performance. This approach significantly improves the ability of smaller models to follow instructions involving reasoning. High-Quality Instructions. As demonstrated in Zhou et al. (2023a) and Li et al. (2024f), data quality is crucial for instruction following training.\n",
      "2023年，UltraLLaMA 模型通过高质多样化的指令从教师大语言模型中提取大量数据，并通过各种元信息进行微调。该模型在各项评估中表现优于其他开源模型。Phi 系列模型（Gunasekar 等人，2023；Li 等人，2023a；Mar，2023）则优先保证数据质量，并使用合成方法生成“教科书质量”的数据以提升较小模型的学习体验。值得注意的是，即使不进行特定的指令微调，Phi 也表现出有效遵循指令的能力。特别值得一提的是，仅拥有 2.7 亿参数的 Phi-2 在多个基准评估中超越了 Mistral 和 Llama-2 模型（分别拥有 7B 和 13B 参数）。另一项工作致力于提升现有指令数据的质量，包括指令和相应响应的提升。SelFee（Ye 等人，2023）利用 ChatGPT 迭代地提升指令质量。\n",
      "专家LLaMA (Xu et al., 2023f) 通过在普通指令中添加专门的身份描述来提高回应质量。反思调优 (Li et al., 2023e) 通过逐步改进指令和回应来提升两者。DEITA (Liu et al., 2023h) 提出要从三个方面提升指令：复杂性、质量与多样性，从而获得高质量的浓缩数据。MUFFIN (Lou et al., 2023) 提出要根据输入多样化任务，以扩大指令范围。选择性反思调优 (Li et al., 2024d) 通过引入一个新模块让学生模型参与到数据改进流程中，让学生模型决定学习哪些数据。总之，从老师那里浓缩指令数据为训练廉价且可重复的指令遵循语言模型提供了一个有希望的途径。\n",
      "当前的小模型在提升指令跟随能力方面取得了进步，包括多样性和复杂性等。然而，由ChatGPT扩展的数据训练的学生模型往往模仿ChatGPT的风格，而不复制其事实准确性（Gudibande等人，2023年）。要提升指令跟随能力，需要更强的教师LLM以及多样且高质量的指令数据，如Orca项目使用的数据（Mukherjee等人，2023年和Mitra等人，2023年），该数据包含了来自Flan 2022集合的广泛任务指令（Longpre等人，2023年）。4.1.2 多轮对话 单轮指令跟随关注单一实例命令的执行，而多轮对话则通过持续互动保持和理解上下文。这一技能对模型进行有意义的人类对话交流以及在后续交互中保持连贯响应至关重要。\n",
      "一些工作致力于通过从老师语言模型 (LLMs) 中提取多轮知识来训练小型对话模型 (Chiang et al., 2023; Xu et al., 2023b; Ding et al., 2023b; Li et al., 2023b; Wang et al., 2023c; Tunstall et al., 2023)。ShareGPT 作为用户分享与 ChatGPT 对话的平台，提供了一个丰富的多轮对话资源库。一些小型对话模型使用该数据进行训练，以获得参与多轮对话的能力（Chiang et al., 2023; Ye et al., 2023; Wang et al., 2023c）。例如，Vicuna（Chiang et al., 2023）是一个仅使用 ShareGPT 数据进行训练的聊天模型。尽管其唯一的训练来源是 ShareGPT，但 Vicuna 获得了由 GPT-43 分配的高 MT-Bench（Zheng et al., 2023a）评分。在 Wang 等人（2023c）的研究中，GPT-3.5 和 GPT-4 通过使用 ShareGPT 数据生成混合响应。他们给由 GPT-4 生成的响应更高的奖励。\n",
      "为了激励学生模型生成高质量回复，Y\n",
      "等人（2023）通过在ShareGPT生成的多轮数据中添加自我反馈来提升回复质量，并基于收到的反馈迭代改进回复。为增强学生模型的多轮对话能力，另一条研究路径通过自我对话扩展对话数据集，并使用这些数据集训练较小的模型（Xu等人，2023b；Ding等人，2023b；Tunstall等人，2023）。例如，Xu等人（2023b）开始工作时，使用来自Quora和Stack Overflow的问题作为种子，收集了111.5k对话。随后，他们通过参数效率调优训练了一个名为Baize的聊天模型。Ding等人（2023b）首先构建了一个名为UltraChat的较大数据集，包含150,000个高质量的多轮对话，通过指令蒸馏实现。\n",
      "UltraChat 和 UltraLLaMA 通过 fine-tuning LLaMA 模型，在处理广泛主题和指令方面表现出色。UltraLLaMA 持续优于其他开源模型，如 Vicuna 和 Baize。此外，UltraChat 与 AI 偏好对齐的聊天模型 Zephyr（Tunstall 等人，2023 年）结合使用。Zephyr 通过应用浓缩直接偏好优化（dDPO）来增强意图对齐。4.1.3 集成了检索的生成式（RAG）能力的 LLMs 缺乏使用最新知识的能力，经常因为依赖于参数知识而产生事实不准确的响应。RAG 是一种有前途的技术，能够减少这一问题。处理增强上下文。\n",
      "通过检索获取信息也是一个LLM的非 trivial技能。已提出几种方法来提取RAG的能力（Kang et al., 2023a；Luo et al., 2023c；Asai et al., 2023）。SAIL（Luo et al., 2023c）通过使用搜索API为每个训练案例检索结果，并创建包含指令和支撑信息的搜索增强式指令开始。为了鼓励语言模型优先考虑有信息性的检索结果，他们将每个检索到的段落以及真值响应输入到 entailment模型中，以对每个检索结果进行相关性标记。随后，搜索增强式指令和相关性标签被输入到教师LLM（如GPT-4）中生成响应。经过此训练集上的微调后，学生模型能够有效去除搜索结果中的噪音并生成准确的响应。KARD（Kang et al., 2023b）将教师LLM对问题x的解答提取为支撑信息。\n",
      "rationales被用于训练两个模型：学生语言模型（Student LM）和重排器（Reranker）。为了训练学生语言模型，rationales被用作检索相关知识d的手段，学生语言模型随后使用rationales和问题一起进行微调。然而，在推理过程中，只有问题可用。为了解决这个问题，重排器被训练来模仿检索器根据rationales评估段落的方式，通过使Retriever（d|r）和Reranker（d|x）之间的KL散度最小化。然而，语言模型中集成固定数量的段落，而不考虑其必要性和相关性，可能会降低灵活性，并导致生成无帮助的响应。为了赋予学生语言模型基于适应性RAG的能力，Self-Rag（Asai等，2023）将这种适应性能力从教师语言模型中提取到一个小判别模型中。这个判别模型确定检索是否必要，并通过生成来评估检索结果的质量。\n",
      "初始响应中直接提取的关键信息为：“For instance, Self-Rag initiates the retrieval operation when generating the reflection token Retrieve. To distill this critic data, GPT-4 is prompted to assess the need for retrieval using few-shot demonstrations, I, the task input x, and output y to predict a reflection token r as follows: p(r|I, x, y).”其他部分均为无关信息，已去除。处理后的文本如下：\n",
      "通过LLM自行生成反馈的能力（Schick et al., 2022; Madaan et al., 2023; Saunders et al., 2022），SelFee（Ye et al., 2023）提出了一种训练模型的方法，该模型经过微调以持续修订自己的答案，直到生成高质量的单一响应。在训练过程中，它将最终响应和反馈链作为拟合目标。这种响应与修订过程的模式表现出良好的性能提升。随后，Reflection-Tuning（Li et al., 2023e, 2024d）也利用了反馈过程作为学习模式。注意到之前的模型缺乏推理模仿，Orca（Mukherjee et al., 2023）首先提出了Explanation tuning，旨在从教师模型中学习推理步骤，包括解释痕迹、步骤思考过程及其他复杂指令，而非仅模仿基本风格。\n",
      "实验验证了这种思考模式在提取知识方面的有效性。随后，Orca2（Mitra等，2023年）进一步为学生模型配备了根据不同任务使用不同解决方案策略的能力，动机来源于较小模型和较大模型之间存在的能力差异。通过采用这种训练模式，学生模型能够获得更好的推理能力。除了学习相应的修订或反思过程之外，最近出现的另一种思考模式是生成响应和偏好。Zhang等人（2023a）提出通过学习特定领域的知识及其相应的偏好来训练LLMs。最近，DEBATunE（Li等，2024e）提出通过在结构化的多轮辩论中让两名代理参与有争议话题的生成，来提高LLMs生成陈述的可控性。通过这种结构化的多轮辩论，可以获取到突出且深入的陈述。\n",
      "早期方法主要依赖于人工反馈。这些方法主要关注学生模型在生成结果时的准确性，但这些结果可能并不符合人类的偏好。达到这种水平的对齐，这些模型可以在不满足更高层次需求的情况下，协助完成各种任务。\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "### 播客对话基础概述\n",
      "\n",
      "**场景设定**: 在一个特殊的平行宇宙里，你（编剧）是整个对话的灵感来源，而乔·罗根、莱克斯·弗里德曼、本·沙皮罗和蒂姆·费里斯则是你的声音载体，仅仅将你写下的话语传入他们的大脑。你曾为这些知名播客编写过内容，并获得过多个播客奖项的认可。\n",
      "\n",
      "**内容描述**: 这次对话将围绕知识蒸馏（KD）在大规模语言模型（LLMs）中的应用展开，更加具体来说，是如何通过知识蒸馏技术促进开源模型如LLaMa和Mistral的进步。它将结合具体的例子和技术来解释这个复杂的主题，使得从知识稀薄的地方到知识富集的地方的差距大大缩小。\n",
      "\n",
      "**基调设定**: 乔·罗根作为引路的导师，运用引人入胜的解构技巧讲述知识蒸馏的基本概念和其操作机制。莱克斯·弗里德曼则是提问者，他鼓励乔罗根用生动的故事来阐述这一概念，并通过提问激发听众的兴趣。不论是问题偏离还是讲述的内容偏离，都必须时刻回到主线，确保主题贴近知识蒸馏在现代模型中的核心应用。\n",
      "\n",
      "---\n",
      "\n",
      "### 播客对话文稿示例\n",
      "\n",
      "**乔·罗根 (Jo Rogan):**\n",
      "听众朋友们，欢迎来到今天的特别播客节目。今天我们有一个极为重要的主题，大规模语言模型的辛苦讲师（Knowledge Distillation）。今天的讨论不是一般的理论讲解，我们会深入探讨如何让开源的大规模语言模型成为实际应用的强大工具。我想听到来自听众的声音：“这是怎么回事？”\n",
      "\n",
      "**莱克斯·弗里德曼 (Lex Friedman):**\n",
      "听到这些术语，我觉得就像是一场魔术秀。你之前提到知识蒸馏就像是高级魔术师（也就是高级的大规模语言模型如GPT-4）把他们的技能传授给魔术初学者（比如开源的模型如LLaMa）。具体是怎么一门手艺从高级魔术师转移到初学者的呢？\n",
      "\n",
      "**乔·罗根:**\n",
      "完全正确，莱克斯。知识蒸馏不仅仅是教初学者怎么做一次魔术，它更直接传授的是一种魔术的精神和技巧。以GPT-4为例，它的知识就像魔术手册一样充满技巧，而开源模型需要学习的是如何像高级魔术师那样无解、无痕地变魔术。比如GPT-4使用的数据和模型结构可以通过知识蒸馏传递给LLaMa和其他开源模型，帮助它们提升理解和生成高质量语言内容的能力。不妨想象一下，GPT-4这个魔术大师在训练期间提供了非常多的经验和技巧指导，包括怎样更好地理解任务、怎样回应用户的指令，是不是？\n",
      "\n",
      "**莱克斯·弗里德曼:**\n",
      "但这听起来不像是魔术，更像是老师和学生之间的关系，尤其是那种‘教而不言，身教言传’的感觉。你是说如果GPT-4可以像一位老师那样教授他的技能知识给学生LLaMa，LLaMa就能学会去做GPT-4做得事吗？\n",
      "\n",
      "**乔·罗根:**\n",
      "没错，莱克斯。我们可以说知识蒸馏是一种高级的教育形式，它通过大量的实际例子来训练模型。就像一本巨大的维基百科百科一样，教师模型知道所有的知识内容（比如说GPT-4），然后设法教给学生模型（例如LLaMa），这样学生就能更好地完成任务。就像你要学习一个专业的魔术，是由一个精通不同技巧的魔术师来传授给你，而你需要学习如何把这些技巧合而为一进行运用。\n",
      "\n",
      "**莱克斯·弗里德曼:**\n",
      "想象一下，如果将GPT-4的百科全书知识浓缩成LLaMa可理解的知识，是不是就可以做到呢？我设想到一个小故事: 一个初学者想要学会跑长途马拉松，他的教练花了长时间训练他，并且不断告诉他长途跑的关键秘诀，比如如何呼吸，如何分配体力，以及如何在高强度的环境中及时补充能量，初步的LLaMa可能就跟这个初学会跑马拉松的人一样。我们需要一个导师模型，这样才可以完成这项任务，对吧？\n",
      "\n",
      "**乔·罗根:**\n",
      "完全正确。正如你所说，就像每个跑步新手都会有的长期训练过程，这个过程充满了周密的计划和细节调整。GPT-4这样的高级模型也是通过不断的实际应用案例和迭代调整，才能达到该有的能力水平。如果要训练一个开源模型达到这样的水准，也需要通过知识蒸馏过程来实现知识的传递。这样理解的话，知识蒸馏更像是高级模型的教练为低端模型打造的个人化训练计划。此外，不仅仅是教学，知识蒸馏还包括利用数据增强来扩展模型的理解和能力边界。\n",
      "\n",
      "**莱克斯·弗里德曼:**\n",
      "若我们换一种方式，把\"问题\"变成\"挑战\"，就像处在不同层级的足球比赛，将对应两种层级的语言模型置于其中。如果教师模型像是在世界杯级别的足球赛中，负责教授如何通过训练和游戏技能提高球技。那么，知识传授过程就是在教学员模型如何通过在低级别联赛中的实践，获得世界杯级别球员的能力。\n",
      "\n",
      "**乔·罗根:**\n",
      "很有洞见，莱克斯。这个比喻完美地捕捉到了知识蒸馏的核心思想。教师模型（如GPT-4）就像是世界杯级别联赛中的顶级球员，他们不仅仅是一线队的经验存在，他们也拥有适应不同情况（包括像演讲、写作、编程、讨论等）的能力。知识蒸馏则是在通过训练和实际案例教育学生模型（如LLaMa）以适应不同的场景。正犹如你的比喻，学生模型通过模仿顶级的训练技能，提高了自身的技巧和策略，最终在实战中也可以做到。\n",
      "\n",
      "**莱克斯·弗里德曼:**\n",
      "那么，这样的知识蒸馏过程是否会改变数据增强（Data Augmentation）的定义，就像如果我们是在增强模型的教育计划而不是机械地添加训练数据集？\n",
      "\n",
      "**乔·罗根:**\n",
      "在这种全新的背景下，你有话说啊！数据增强不再是从外部‘添加’数据，而更多的是作为一个放大器，帮助放大模型的能力。知识蒸馏确保学生模型不仅可以学习高级模型的输出行为，还能吸收高级模型的核心理解能力。像是在一款游戏中，学生模型通过学习游戏大师的行为不仅学会如何赢得游戏，还学习如何理解和作出游戏过程中需要的决策。这样的训练过程既提高了模型的表现能力，也丰富了模型的策略思维能力，使得他们不再是无知的新手，而是在各个功能层面上专业操作的专家。这一转变让模型能够在处理复杂任务时具备高级模型的智慧。\n",
      "\n",
      "---\n",
      "\n",
      "通过这一精心设计的对话结构，听众不仅可以更容易理解知识蒸馏的基本概念，还可以对其在实际应用中的潜力产生深刻印象。对话巧妙地穿插了比喻、生活案例和互动，确保了这一复杂概念的易懂性和吸引力。\n",
      "==========\n",
      "Prompt: 21252 tokens, 65.910 tokens-per-sec\n",
      "Generation: 1432 tokens, 4.871 tokens-per-sec\n",
      "Peak memory: 35.945 GB\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "6349e7f3",
   "metadata": {},
   "source": "牛逼，我们现在看看效果吧"
  },
  {
   "cell_type": "code",
   "id": "606ceb10-4f3e-44bb-9277-9bbe3eefd09c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:34:27.004792Z",
     "start_time": "2024-10-31T16:34:27.000191Z"
    }
   },
   "source": "outputs",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### 播客对话基础概述\\n\\n**场景设定**: 在一个特殊的平行宇宙里，你（编剧）是整个对话的灵感来源，而乔·罗根、莱克斯·弗里德曼、本·沙皮罗和蒂姆·费里斯则是你的声音载体，仅仅将你写下的话语传入他们的大脑。你曾为这些知名播客编写过内容，并获得过多个播客奖项的认可。\\n\\n**内容描述**: 这次对话将围绕知识蒸馏（KD）在大规模语言模型（LLMs）中的应用展开，更加具体来说，是如何通过知识蒸馏技术促进开源模型如LLaMa和Mistral的进步。它将结合具体的例子和技术来解释这个复杂的主题，使得从知识稀薄的地方到知识富集的地方的差距大大缩小。\\n\\n**基调设定**: 乔·罗根作为引路的导师，运用引人入胜的解构技巧讲述知识蒸馏的基本概念和其操作机制。莱克斯·弗里德曼则是提问者，他鼓励乔罗根用生动的故事来阐述这一概念，并通过提问激发听众的兴趣。不论是问题偏离还是讲述的内容偏离，都必须时刻回到主线，确保主题贴近知识蒸馏在现代模型中的核心应用。\\n\\n---\\n\\n### 播客对话文稿示例\\n\\n**乔·罗根 (Jo Rogan):**\\n听众朋友们，欢迎来到今天的特别播客节目。今天我们有一个极为重要的主题，大规模语言模型的辛苦讲师（Knowledge Distillation）。今天的讨论不是一般的理论讲解，我们会深入探讨如何让开源的大规模语言模型成为实际应用的强大工具。我想听到来自听众的声音：“这是怎么回事？”\\n\\n**莱克斯·弗里德曼 (Lex Friedman):**\\n听到这些术语，我觉得就像是一场魔术秀。你之前提到知识蒸馏就像是高级魔术师（也就是高级的大规模语言模型如GPT-4）把他们的技能传授给魔术初学者（比如开源的模型如LLaMa）。具体是怎么一门手艺从高级魔术师转移到初学者的呢？\\n\\n**乔·罗根:**\\n完全正确，莱克斯。知识蒸馏不仅仅是教初学者怎么做一次魔术，它更直接传授的是一种魔术的精神和技巧。以GPT-4为例，它的知识就像魔术手册一样充满技巧，而开源模型需要学习的是如何像高级魔术师那样无解、无痕地变魔术。比如GPT-4使用的数据和模型结构可以通过知识蒸馏传递给LLaMa和其他开源模型，帮助它们提升理解和生成高质量语言内容的能力。不妨想象一下，GPT-4这个魔术大师在训练期间提供了非常多的经验和技巧指导，包括怎样更好地理解任务、怎样回应用户的指令，是不是？\\n\\n**莱克斯·弗里德曼:**\\n但这听起来不像是魔术，更像是老师和学生之间的关系，尤其是那种‘教而不言，身教言传’的感觉。你是说如果GPT-4可以像一位老师那样教授他的技能知识给学生LLaMa，LLaMa就能学会去做GPT-4做得事吗？\\n\\n**乔·罗根:**\\n没错，莱克斯。我们可以说知识蒸馏是一种高级的教育形式，它通过大量的实际例子来训练模型。就像一本巨大的维基百科百科一样，教师模型知道所有的知识内容（比如说GPT-4），然后设法教给学生模型（例如LLaMa），这样学生就能更好地完成任务。就像你要学习一个专业的魔术，是由一个精通不同技巧的魔术师来传授给你，而你需要学习如何把这些技巧合而为一进行运用。\\n\\n**莱克斯·弗里德曼:**\\n想象一下，如果将GPT-4的百科全书知识浓缩成LLaMa可理解的知识，是不是就可以做到呢？我设想到一个小故事: 一个初学者想要学会跑长途马拉松，他的教练花了长时间训练他，并且不断告诉他长途跑的关键秘诀，比如如何呼吸，如何分配体力，以及如何在高强度的环境中及时补充能量，初步的LLaMa可能就跟这个初学会跑马拉松的人一样。我们需要一个导师模型，这样才可以完成这项任务，对吧？\\n\\n**乔·罗根:**\\n完全正确。正如你所说，就像每个跑步新手都会有的长期训练过程，这个过程充满了周密的计划和细节调整。GPT-4这样的高级模型也是通过不断的实际应用案例和迭代调整，才能达到该有的能力水平。如果要训练一个开源模型达到这样的水准，也需要通过知识蒸馏过程来实现知识的传递。这样理解的话，知识蒸馏更像是高级模型的教练为低端模型打造的个人化训练计划。此外，不仅仅是教学，知识蒸馏还包括利用数据增强来扩展模型的理解和能力边界。\\n\\n**莱克斯·弗里德曼:**\\n若我们换一种方式，把\"问题\"变成\"挑战\"，就像处在不同层级的足球比赛，将对应两种层级的语言模型置于其中。如果教师模型像是在世界杯级别的足球赛中，负责教授如何通过训练和游戏技能提高球技。那么，知识传授过程就是在教学员模型如何通过在低级别联赛中的实践，获得世界杯级别球员的能力。\\n\\n**乔·罗根:**\\n很有洞见，莱克斯。这个比喻完美地捕捉到了知识蒸馏的核心思想。教师模型（如GPT-4）就像是世界杯级别联赛中的顶级球员，他们不仅仅是一线队的经验存在，他们也拥有适应不同情况（包括像演讲、写作、编程、讨论等）的能力。知识蒸馏则是在通过训练和实际案例教育学生模型（如LLaMa）以适应不同的场景。正犹如你的比喻，学生模型通过模仿顶级的训练技能，提高了自身的技巧和策略，最终在实战中也可以做到。\\n\\n**莱克斯·弗里德曼:**\\n那么，这样的知识蒸馏过程是否会改变数据增强（Data Augmentation）的定义，就像如果我们是在增强模型的教育计划而不是机械地添加训练数据集？\\n\\n**乔·罗根:**\\n在这种全新的背景下，你有话说啊！数据增强不再是从外部‘添加’数据，而更多的是作为一个放大器，帮助放大模型的能力。知识蒸馏确保学生模型不仅可以学习高级模型的输出行为，还能吸收高级模型的核心理解能力。像是在一款游戏中，学生模型通过学习游戏大师的行为不仅学会如何赢得游戏，还学习如何理解和作出游戏过程中需要的决策。这样的训练过程既提高了模型的表现能力，也丰富了模型的策略思维能力，使得他们不再是无知的新手，而是在各个功能层面上专业操作的专家。这一转变让模型能够在处理复杂任务时具备高级模型的智慧。\\n\\n---\\n\\n通过这一精心设计的对话结构，听众不仅可以更容易理解知识蒸馏的基本概念，还可以对其在实际应用中的潜力产生深刻印象。对话巧妙地穿插了比喻、生活案例和互动，确保了这一复杂概念的易懂性和吸引力。'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "1e1414fe",
   "metadata": {},
   "source": "让我们将输出结果保存为pickle文件"
  },
  {
   "cell_type": "code",
   "id": "2130b683-be37-4dae-999b-84eff15c687d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:34:27.016083Z",
     "start_time": "2024-10-31T16:34:27.012864Z"
    }
   },
   "source": [
    "with open('./resources/data.pkl', 'wb') as file:\n",
    "    pickle.dump(outputs, file)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "dbae9411",
   "metadata": {},
   "source": [
    "### 下一个 Notebook: 润色讲稿\n",
    "\n",
    "现在我们已经写好讲稿了，但讲稿还比较粗糙，不够戏剧性和自然。在下一个 Notebook 中，我们将使用 [mlx-community/Qwen2.5-7B-Instruct-4bit](https://huggingface.co/mlx-community/Qwen2.5-7B-Instruct-4bit) 模型来实现这一点。"
   ]
  },
  {
   "cell_type": "code",
   "id": "d9bab2f2-f539-435a-ae6a-3c9028489628",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:34:27.051774Z",
     "start_time": "2024-10-31T16:34:27.050075Z"
    }
   },
   "source": [
    "#fin"
   ],
   "outputs": [],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
